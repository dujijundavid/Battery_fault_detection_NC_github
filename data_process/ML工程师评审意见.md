# MLå·¥ç¨‹å¸ˆè¯„å®¡æ„è§ï¼šæ•°æ®å¤„ç†è„šæœ¬ç²¾ç®€æ–¹æ¡ˆ

> **è¯„å®¡è§’è‰²**: Senior ML Engineer / MLOpsä¸“å®¶  
> **è¯„å®¡åŸåˆ™**: KISS (Keep It Simple, Stupid) + YAGNI (You Aren't Gonna Need It)  
> **è¯„å®¡æ—¥æœŸ**: 2025-11-26

---

## ğŸ¯ æ ¸å¿ƒè§‚ç‚¹

**åŸåˆ†ææ–‡æ¡£çš„é—®é¢˜ï¼š**
1. âŒ è¿‡åº¦è®¾è®¡ - æå‡ºçš„æ¨¡å—åŒ–æ–¹æ¡ˆå¤ªå¤æ‚
2. âŒ ä¸åˆ‡å®é™… - å¾ˆå¤š"å·¥å…·è„šæœ¬"æ²¡æœ‰å¿…è¦ä¿ç•™
3. âŒ é”™å¤±é‡ç‚¹ - æ²¡æœ‰æŠ“ä½çœŸæ­£çš„é—®é¢˜æ‰€åœ¨

**æœ¬è¯„å®¡çš„å»ºè®®ï¼š**
1. âœ… **åªä¿ç•™ 2 ä¸ªè„šæœ¬** - `data_process.py` + `config.yaml`
2. âœ… **çœç•¥æ‰€æœ‰"å·¥å…·è„šæœ¬"** - ç”¨Jupyter notebookæˆ–å‘½ä»¤è¡Œæ›¿ä»£
3. âœ… **ç»Ÿä¸€æ‰§è¡Œå¼•æ“** - åªç”¨Sparkï¼Œåˆ«æå¤šä¸ªç‰ˆæœ¬

---

## ğŸ“Š è„šæœ¬ä¿ç•™/åˆ é™¤å†³ç­–è¡¨

| è„šæœ¬åç§°                           | å½“å‰çŠ¶æ€ | è¯„å®¡å†³ç­–       | ç†ç”±                                                   |
| ---------------------------------- | -------- | -------------- | ------------------------------------------------------ |
| `0ã€read_parquet.py`               | 23è¡Œ     | âŒ **åˆ é™¤**     | `spark.read.parquet().show()` ä¸€è¡Œæå®šï¼Œä¸éœ€è¦å•ç‹¬è„šæœ¬ |
| `0ã€read_pkl.py`                   | 47è¡Œ     | âŒ **åˆ é™¤**     | è°ƒè¯•æ—¶ç”¨Jupyter cellï¼Œä¸éœ€è¦ç‹¬ç«‹è„šæœ¬                   |
| `æŸ¥çœ‹parquetçš„åˆ—åæ˜¯å¦ç¬¦åˆé¢„æœŸ.py` | 131è¡Œ    | âŒ **åˆ é™¤**     | `df.columns` æˆ– `df.printSchema()` å³å¯                |
| `åˆå¹¶.py`                          | 140è¡Œ    | âš ï¸ **å•æ¬¡è¿è¡Œ** | æ•°æ®é¢„å¤„ç†åªéœ€è¿è¡Œä¸€æ¬¡ï¼Œä¸éœ€è¦ç»´æŠ¤                     |
| `é’ˆå¯¹æ•…éšœè½¦çš„å¤„ç†.py`              | 481è¡Œ    | âœ… **æ•´åˆ**     | ä¸»è¦é€»è¾‘åˆå¹¶åˆ°ä¸»è„šæœ¬                                   |
| `test_data_process.py`             | 418è¡Œ    | âŒ **åˆ é™¤**     | ä¸ºä»€ä¹ˆè¦ç»´æŠ¤å¤šä¸ªç‰ˆæœ¬ï¼Ÿ                                 |
| `test_data_process_new.py`         | 488è¡Œ    | âŒ **åˆ é™¤**     | åŒä¸Š                                                   |
| `test_data_process_spark.py`       | 325è¡Œ    | âœ… **ä¿ç•™åŸºç¡€** | ç®€åŒ–åä½œä¸ºä¸»è„šæœ¬                                       |
| `train_data_process.py`            | 427è¡Œ    | âŒ **åˆ é™¤**     | ä¸testé€»è¾‘ä¸€æ ·ï¼Œå‚æ•°åŒºåˆ†å³å¯                           |
| `folder_mapping.csv`               | é…ç½®     | âœ… **ä¿ç•™**     | å¦‚éœ€è¦çš„è¯                                             |

**ç»“è®ºï¼š10ä¸ªè„šæœ¬ â†’ ä¿ç•™1ä¸ª + é‡å†™ä¸ºæ¸…æ™°ç‰ˆæœ¬**

---

## ğŸ’¡ å®ç”¨é‡æ„æ–¹æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼‰

### æ–¹æ¡ˆï¼šå•ä¸€è„šæœ¬ + é…ç½®æ–‡ä»¶

```
data_process/
â”œâ”€â”€ config.yaml              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data_process.py          # å”¯ä¸€çš„æ•°æ®å¤„ç†è„šæœ¬
â”œâ”€â”€ README.md                # ç®€å•ä½¿ç”¨è¯´æ˜
â””â”€â”€ notebooks/               # å¯é€‰ï¼šæ•°æ®æ¢æŸ¥ç”¨
    â””â”€â”€ explore_data.ipynb   # Jupyter notebookæŸ¥çœ‹æ•°æ®
```

---

## ğŸ“ `data_process.py` - ç²¾ç®€ç‰ˆè®¾è®¡

### æ ¸å¿ƒåŸåˆ™

1. **åªç”¨Spark** - åˆ«æ•´ä»€ä¹ˆmultiprocessingï¼ŒSparkå°±æ˜¯ä¸ºäº†åˆ†å¸ƒå¼å¤„ç†
2. **ä¸€ä¸ªè„šæœ¬æå®š** - train/testç”¨å‚æ•°åŒºåˆ†ï¼Œåˆ«å†™ä¸¤é
3. **é…ç½®å¤–ç½®** - æ‰€æœ‰å‚æ•°æ”¾config.yaml
4. **ç®€å•ç›´æ¥** - 500è¡Œä»¥å†…æå®šæ‰€æœ‰é€»è¾‘

### ä»£ç ç»“æ„ï¼ˆä¼ªä»£ç ï¼‰

```python
# data_process.py
from pyspark.sql import SparkSession
import yaml

def load_config(path):
    """è¯»å–é…ç½®æ–‡ä»¶"""
    with open(path) as f:
        return yaml.safe_load(f)

def clean_data(df):
    """æ•°æ®æ¸…æ´— - 200è¡Œä»¥å†…"""
    # ä½ çš„ç”¨æˆ·è¯´å¾—å¯¹ï¼šspark.read.parquet() å°±èƒ½è¯»ï¼Œä¸éœ€è¦å¤æ‚çš„åˆ—æ˜ å°„
    # ç›´æ¥åœ¨è¿™é‡Œé‡å‘½åå’Œæ¸…æ´—
    df = df.withColumnRenamed("bms_total_voltage", "volt") \
           .withColumn("volt", col("volt") / col("cells"))
    return df.filter(...)  # å„ç§è¿‡æ»¤æ¡ä»¶

def identify_charging_segments(df):
    """è¯†åˆ«å……ç”µæ®µ - ä½¿ç”¨Windowå‡½æ•°ï¼Œ50è¡Œä»¥å†…"""
    window = Window.partitionBy("car_id").orderBy("time")
    df = df.withColumn("time_diff", col("time") - lag("time").over(window))
    df = df.withColumn("segment_id", 
                       sum(when(col("time_diff") > 300, 1).otherwise(0)).over(window))
    return df

def create_sliding_windows(df, window_len=40, interval=2):
    """ç”Ÿæˆæ»‘åŠ¨çª—å£ - ä½¿ç”¨Spark SQLï¼Œ100è¡Œä»¥å†…"""
    # ç”¨collect_list + window functions
    # ä¸éœ€è¦è‡ªå·±å†™å¤æ‚çš„å¾ªç¯é€»è¾‘
    pass

def save_as_pkl(df, output_path):
    """ä¿å­˜ç»“æœ - ç”¨mapPartitionsè½¬PKLï¼Œ50è¡Œä»¥å†…"""
    def to_pkl(iterator):
        import torch
        for row in iterator:
            # è½¬æ¢é€»è¾‘
            yield pkl_data
    
    df.mapPartitions(to_pkl).write.format("...").save(output_path)

def main():
    config = load_config("config.yaml")
    spark = SparkSession.builder.getOrCreate()
    
    # ç®€å•3æ­¥
    df = spark.read.parquet(config['input_path'])  # å°±è¿™ä¹ˆç®€å•ï¼
    df = clean_data(df)
    df = identify_charging_segments(df)
    windows = create_sliding_windows(df)
    save_as_pkl(windows, config['output_path'])
    
    print("Done!")

if __name__ == "__main__":
    main()
```

**æ€»ä»£ç é‡ï¼š~500 è¡Œï¼ˆvs åŸæ¥çš„ 2500+ è¡Œï¼‰**

---

## ğŸ”§ `config.yaml` - é…ç½®æ–‡ä»¶è®¾è®¡

```yaml
# æ•°æ®è·¯å¾„
data:
  input: /Volumes/.../203_Validation_new/
  output: /Volumes/.../pkl_output/
  
# æ•°æ®é›†é…ç½®
dataset:
  type: "test"  # or "train"
  label: "0"    # æ­£å¸¸æ ·æœ¬=0, å¼‚å¸¸æ ·æœ¬=1
  
  # æ—¶é—´èŒƒå›´ï¼ˆç›¸å¯¹äºé”šç‚¹ï¼‰
  time_range:
    end_months_ago: 0
    cover_months: 12
  
  # è½¦è¾†ç™½åå•ï¼ˆå¯é€‰ï¼‰
  car_whitelist: []  # ç©º=å¤„ç†æ‰€æœ‰è½¦è¾†
  
# ç‰¹å¾å·¥ç¨‹å‚æ•°
features:
  window_length: 40
  window_interval: 2
  sampling_rate: 30  # ç§’
  
  # å……ç”µæ®µè¯†åˆ«
  charging:
    status: "CHARGING_IN_PARKING"
    current_threshold: 0.01
    gap_threshold: 300  # ç§’
    min_points: 25
  
  # è¿‡æ»¤æ¡ä»¶
  filters:
    min_mileage: 1000.0
    soc_range: [0, 100]
    
# Sparké…ç½®
spark:
  app_name: "BatteryDataProcessing"
  shuffle_partitions: 200
  executor_memory: "4g"
  
# è¾“å‡ºé…ç½®
output:
  format: "pkl"  # or "delta" for easier querying
  partition_by: "car_id"
```

---

## âŒ ä¸éœ€è¦çš„åŠŸèƒ½/è„šæœ¬

### 1. æ‰€æœ‰"æŸ¥çœ‹æ•°æ®"çš„è„šæœ¬

**åŸå› ï¼š**
- Jupyter notebook æ›´é€‚åˆæ¢ç´¢æ€§åˆ†æ
- ç”Ÿäº§ç¯å¢ƒä¸éœ€è¦è¿™äº›å•æ¬¡ä½¿ç”¨çš„è„šæœ¬

**æ›¿ä»£æ–¹æ¡ˆï¼š**
```python
# åœ¨ notebooks/explore_data.ipynb é‡Œ
df = spark.read.parquet("/path/to/data")
df.show()
df.printSchema()
df.describe().show()
```

---

### 2. å¤šä¸ªç‰ˆæœ¬çš„æ•°æ®å¤„ç†è„šæœ¬

**åŸå› ï¼š**
- ç»´æŠ¤å¤šä¸ªç‰ˆæœ¬æ˜¯å™©æ¢¦
- Sparkæœ¬èº«å°±æ”¯æŒæœ¬åœ°/é›†ç¾¤æ¨¡å¼æ— ç¼åˆ‡æ¢
- æ€§èƒ½é—®é¢˜ç”¨Sparké…ç½®è°ƒä¼˜è§£å†³ï¼Œä¸æ˜¯é‡å†™ä»£ç 

**é”™è¯¯æ€ç»´ï¼š**
```
æœ¬åœ°ç‰ˆæœ¬ â†’ multiprocessing.Pool
Databricksç‰ˆæœ¬ â†’ pandas + è¿›ç¨‹æ±   
Sparkç‰ˆæœ¬ â†’ PySpark
```

**æ­£ç¡®æ€ç»´ï¼š**
```
åªç”¨ä¸€ä¸ªSparkç‰ˆæœ¬ï¼š
- æœ¬åœ°æµ‹è¯•: spark-submit --master local[*]
- é›†ç¾¤è¿è¡Œ: spark-submit --master yarn
```

---

### 3. è¿‡åº¦è®¾è®¡çš„"æ¨¡å—åŒ–"

**åŸåˆ†ææå‡ºçš„ï¼š**
```
data_utils.py      # æ ¸å¿ƒåº“
data_pipeline.py   # ç®¡é“ç±»
data_tools.py      # å·¥å…·é›†
config.yaml        # é…ç½®
```

**é—®é¢˜ï¼š**
- å¯¹äºä¸€ä¸ªæ•°æ®å¤„ç†ä»»åŠ¡æ¥è¯´ï¼Œè¿™æ˜¯è¿‡åº¦è®¾è®¡
- æœªæ¥ç»´æŠ¤è€…éœ€è¦åœ¨4ä¸ªæ–‡ä»¶é—´è·³è½¬
- å¢åŠ äº†è®¤çŸ¥è´Ÿæ‹…

**ç®€åŒ–åï¼š**
```
data_process.py    # å•ä¸€è„šæœ¬ï¼Œæ‰€æœ‰é€»è¾‘éƒ½åœ¨è¿™é‡Œ
config.yaml        # é…ç½®æ–‡ä»¶
```

---

### 4. å¤æ‚çš„"30ç§’è§„èŒƒåŒ–"é€»è¾‘

**å½“å‰å®ç°ï¼š**
- æ£€æŸ¥æ—¶é—´é—´éš”
- æ‰‹åŠ¨æ’å€¼
- å¤„ç†å„ç§è¾¹ç•Œæƒ…å†µ
- ~100è¡Œä»£ç 

**ç®€åŒ–æ–¹æ¡ˆï¼š**
```python
# ç”¨Sparkçš„æ—¶é—´çª—å£åŠŸèƒ½ - 10è¡Œæå®š
from pyspark.sql.functions import window

df.groupBy(
    window("timestamp", "30 seconds"),
    "car_id"
).agg(
    avg("voltage").alias("volt"),
    avg("current").alias("current"),
    # ... å…¶ä»–ç‰¹å¾
)
```

**ä¸ºä»€ä¹ˆå¯ä»¥ç®€åŒ–ï¼Ÿ**
- ä¸å®Œç¾çš„å¯¹é½ä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½
- å¦‚æœçœŸçš„éœ€è¦ç²¾ç¡®å¯¹é½ï¼Œæ¨¡å‹è®­ç»ƒæ—¶å¯ä»¥åšdata augmentation
- è¿‡åº¦æ‹Ÿåˆæ•°æ®å¤„ç†æµç¨‹åè€Œé™ä½é²æ£’æ€§

---

## ğŸš€ å®æ–½å»ºè®®

### ç¬¬1å¤©ï¼šæ¸…ç†å·¥ä½œ

```bash
# 1. å¤‡ä»½å½“å‰è„šæœ¬
mkdir deprecated_scripts
mv *.py deprecated_scripts/

# 2. åªä¿ç•™éœ€è¦çš„
# ï¼ˆå®é™…ä¸Šä»€ä¹ˆéƒ½ä¸ä¿ç•™ï¼Œé‡æ–°å†™ï¼‰
```

### ç¬¬2-3å¤©ï¼šç¼–å†™æ–°è„šæœ¬

**`data_process.py` ç¼–å†™ä¼˜å…ˆçº§ï¼š**

1. âœ… **å…ˆå†™æœ€ç®€ç‰ˆæœ¬** - èƒ½è·‘é€šå³å¯ï¼ˆ1å¤©ï¼‰
   - è¯»å–æ•°æ®
   - åŸºç¡€æ¸…æ´—
   - ä¿å­˜ç»“æœ

2. âœ… **æ·»åŠ æ ¸å¿ƒé€»è¾‘** - å……ç”µæ®µè¯†åˆ«ã€çª—å£ç”Ÿæˆï¼ˆ1å¤©ï¼‰
   - ä¸è¦è¿½æ±‚å®Œç¾
   - èƒ½ç”Ÿæˆè®­ç»ƒæ•°æ®å³å¯

3. âš ï¸ **å¯é€‰ä¼˜åŒ–** - æ€§èƒ½è°ƒä¼˜ï¼ˆæ ¹æ®éœ€è¦ï¼‰
   - å¦‚æœæ•°æ®é‡å°ï¼Œä¸éœ€è¦ä¼˜åŒ–
   - å¦‚æœçœŸçš„æ…¢ï¼Œè°ƒSparkå‚æ•°

### ç¬¬4å¤©ï¼šæµ‹è¯•éªŒè¯

```bash
# æœ¬åœ°æµ‹è¯•ï¼ˆå°æ•°æ®é›†ï¼‰
spark-submit --master local[4] data_process.py --config config_test.yaml

# é›†ç¾¤è¿è¡Œï¼ˆå…¨é‡æ•°æ®ï¼‰
spark-submit --master yarn --deploy-mode cluster data_process.py --config config.yaml
```

### ç¬¬5å¤©ï¼šæ–‡æ¡£

**README.mdï¼ˆä¸è¦å†™å¤ªå¤šï¼‰ï¼š**

```markdown
# ç”µæ± æ•°æ®å¤„ç†

## å¿«é€Ÿå¼€å§‹

1. ä¿®æ”¹é…ç½®: `vim config.yaml`
2. è¿è¡Œ: `spark-submit data_process.py`
3. è¾“å‡ºåœ¨: `config.yaml` ä¸­æŒ‡å®šçš„è·¯å¾„

## é…ç½®è¯´æ˜

è§ config.yaml æ³¨é‡Š

## å¸¸è§é—®é¢˜

Q: å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
A: å¢åŠ  `spark.shuffle_partitions`

Q: å†…å­˜ä¸è¶³ï¼Ÿ
A: å¢åŠ  `spark.executor.memory`
```

**å°±è¿™ä¹ˆå¤šï¼Œåˆ«å†™é•¿ç¯‡å¤§è®ºï¼**

---

## ğŸ“ ä»£ç è´¨é‡æ ‡å‡†

### âœ… å¥½çš„ä»£ç 

```python
def clean_voltage(df):
    """æ¸…æ´—ç”µå‹æ•°æ®"""
    return df.filter(col("voltage") > 0) \
             .withColumn("voltage", col("voltage") / col("cells"))
```

**ç‰¹ç‚¹ï¼š**
- åŠŸèƒ½å•ä¸€
- å‘½åæ¸…æ™°
- é“¾å¼è°ƒç”¨
- æ— éœ€æ³¨é‡Šå°±èƒ½çœ‹æ‡‚

---

### âŒ åçš„ä»£ç 

```python
def clean_and_select_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æ¸…æ´—æ•°æ®å¹¶é€‰æ‹©å¿…è¦åˆ—"""
    miss = [c for c in COL_MAP if c not in df.columns]
    if miss:
        raise KeyError(f"ç¼ºå¤±å¿…é¡»å­—æ®µ: {miss}")
    x = df[list(COL_MAP.keys())].rename(columns=COL_MAP)
    t_ms = pd.to_numeric(x['time'], errors='coerce').astype('Int64')
    x['time_ms'] = t_ms
    x['time'] = (t_ms // 1000).astype('Int64')
    # ... è¿˜æœ‰40è¡Œ
```

**é—®é¢˜ï¼š**
- å‡½æ•°å¤ªé•¿ï¼ˆ>50è¡Œï¼‰
- åšå¤ªå¤šäº‹æƒ…
- ç±»å‹è½¬æ¢å¤ªå¤æ‚
- æ··ç”¨pandaså’Œsparkæ¦‚å¿µ

**æ”¹è¿›ï¼š**
```python
def clean_data(df):
    """æ•°æ®æ¸…æ´—"""
    return (df
        .withColumn("time", col("time").cast("bigint") / 1000)
        .withColumn("voltage", col("voltage") / col("cells"))
        .filter(col("voltage") > 0)
        .filter(col("soc").between(0, 100))
    )
```

---

## ğŸ“ MLå·¥ç¨‹æœ€ä½³å®è·µ

### 1. **æ•°æ®å¤„ç†åº”è¯¥æ˜¯æ— çŠ¶æ€çš„**

âŒ **é”™è¯¯ï¼š**
```python
class DataPipeline:
    def __init__(self):
        self.car_ranges = {}
        self.processed_count = 0
    
    def process_car(self, car):
        # ä¿®æ”¹å†…éƒ¨çŠ¶æ€
        self.processed_count += 1
```

âœ… **æ­£ç¡®ï¼š**
```python
def process_cars(cars, config):
    """çº¯å‡½æ•°ï¼šç›¸åŒè¾“å…¥ = ç›¸åŒè¾“å‡º"""
    return spark.read.parquet(...).transform(clean_data)
```

---

### 2. **é…ç½®åº”è¯¥æ˜¯å£°æ˜å¼çš„**

âŒ **é”™è¯¯ï¼š**
```python
# ç¡¬ç¼–ç åœ¨ä»£ç é‡Œ
WINDOW_LEN = 40
INTERVAL = 2
```

âœ… **æ­£ç¡®ï¼š**
```yaml
# config.yaml
features:
  window_length: 40
  window_interval: 2
```

---

### 3. **åˆ«è¿‡æ—©ä¼˜åŒ–**

**å½“å‰è„šæœ¬çš„é—®é¢˜ï¼š**
- åŒæ—¶ç»´æŠ¤ multiprocessingã€pandasã€spark ä¸‰ä¸ªç‰ˆæœ¬
- "ä¼˜åŒ–"çš„ä»£ç æ¯”åŸå§‹ä»£ç è¿˜è¦å¤æ‚

**æ­£ç¡®æ€è·¯ï¼š**
1. å…ˆç”¨Sparkå†™å‡ºèƒ½è·‘çš„ç‰ˆæœ¬
2. å¦‚æœæ…¢ï¼ŒProfileæ‰¾ç“¶é¢ˆ
3. åªä¼˜åŒ–çœŸæ­£çš„ç“¶é¢ˆ
4. ä¼˜åŒ–æ‰‹æ®µï¼šè°ƒå‚æ•° > æ”¹ç®—æ³• > é‡å†™å¼•æ“

---

### 4. **æ•°æ®è´¨é‡ > æ•°æ®å¤„ç†æµç¨‹**

**å½“å‰é‡ç‚¹ï¼š**
- 30ç§’ç²¾ç¡®å¯¹é½
- å¤æ‚çš„æ’å€¼é€»è¾‘
- å„ç§è¾¹ç•Œæƒ…å†µå¤„ç†

**åº”è¯¥å…³æ³¨ï¼š**
- **æ•°æ®æ˜¯å¦æœ‰æ ‡ç­¾ï¼Ÿ**
- **æ­£è´Ÿæ ·æœ¬æ˜¯å¦å¹³è¡¡ï¼Ÿ**
- **ç‰¹å¾æ˜¯å¦æœ‰åŒºåˆ†åº¦ï¼Ÿ**
- **æ˜¯å¦æœ‰æ•°æ®æ³„éœ²ï¼Ÿ**

> æŠŠæ—¶é—´èŠ±åœ¨æ•°æ®è´¨é‡åˆ†æä¸Šï¼Œè€Œä¸æ˜¯è¿‡åº¦è®¾è®¡æµç¨‹ï¼

---

## ğŸ“Š å…³é”®æŒ‡æ ‡å¯¹æ¯”

| ç»´åº¦         | å½“å‰çŠ¶æ€      | å»ºè®®æ–¹æ¡ˆ       | æ”¹è¿› |
| ------------ | ------------- | -------------- | ---- |
| **è„šæœ¬æ•°é‡** | 10ä¸ª          | 1ä¸ª            | -90% |
| **ä»£ç è¡Œæ•°** | 2500+         | ~500           | -80% |
| **ç»´æŠ¤æˆæœ¬** | é«˜ï¼ˆ5ä»½é‡å¤ï¼‰ | ä½ï¼ˆå•ä¸€ç‰ˆæœ¬ï¼‰ | -80% |
| **ä¸Šæ‰‹æ—¶é—´** | 2-3å¤©         | 1å°æ—¶          | -95% |
| **æ‰§è¡Œå¼•æ“** | 3ç§           | 1ç§ï¼ˆSparkï¼‰   | ç»Ÿä¸€ |
| **é…ç½®æ–¹å¼** | ç¡¬ç¼–ç +å‚æ•°   | çº¯é…ç½®æ–‡ä»¶     | æ¸…æ™° |

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### ç«‹å³å¯åšçš„

1. âœ… **åˆ é™¤æ‰€æœ‰"å·¥å…·è„šæœ¬"**
   - `0ã€read_parquet.py` âŒ
   - `0ã€read_pkl.py` âŒ  
   - `æŸ¥çœ‹parquetçš„åˆ—åæ˜¯å¦ç¬¦åˆé¢„æœŸ.py` âŒ
   
   â†’ ç”¨Jupyter notebookæ›¿ä»£

2. âœ… **åˆå¹¶å¤„ç†è„šæœ¬ä¸ºä¸€ä¸ª**
   - é€‰æ‹© `test_data_process_spark.py` ä½œä¸ºåŸºç¡€
   - ç®€åŒ–åé‡å‘½åä¸º `data_process.py`
   - åˆ é™¤å…¶ä»–4ä¸ªç‰ˆæœ¬

3. âœ… **å¤–ç½®é…ç½®**
   - åˆ›å»º `config.yaml`
   - ç§»é™¤æ‰€æœ‰ç¡¬ç¼–ç å‚æ•°

---

### å¯ä»¥æ¨è¿Ÿçš„

1. â¸ï¸ **æ€§èƒ½ä¼˜åŒ–**
   - å…ˆè·‘èµ·æ¥å†è¯´
   - å¦‚æœæ€§èƒ½å¤Ÿç”¨ï¼Œå°±ä¸è¦åŠ¨

2. â¸ï¸ **å®Œå–„æ–‡æ¡£**
   - READMEä¸€é¡µå¤Ÿäº†
   - ä»£ç å³æ–‡æ¡£

3. â¸ï¸ **å•å…ƒæµ‹è¯•**
   - æ•°æ®å¤„ç†è„šæœ¬çš„æµ‹è¯•ROIä¸é«˜
   - ä¸å¦‚æŠŠæ—¶é—´èŠ±åœ¨æ¨¡å‹å®éªŒä¸Š

---

### ä¸è¦åšçš„

1. âŒ **ä¸è¦è®¾è®¡"æ¡†æ¶"**
   - è¿™åªæ˜¯ä¸€ä¸ªæ•°æ®å¤„ç†è„šæœ¬
   - ä¸æ˜¯è¦å‘å¸ƒçš„PythonåŒ…
   
2. âŒ **ä¸è¦è¿‡åº¦æŠ½è±¡**
   - `DataPipeline` åŸºç±»ï¼Ÿä¸éœ€è¦
   - ç­–ç•¥æ¨¡å¼ï¼Ÿä¸éœ€è¦
   - å·¥å‚æ–¹æ³•ï¼Ÿä¸éœ€è¦

3. âŒ **ä¸è¦ç»´æŠ¤å¤šä¸ªç‰ˆæœ¬**
   - ä¸€ä¸ªSparkç‰ˆæœ¬è¶³å¤Ÿ
   - æœ¬åœ°/é›†ç¾¤ç”¨å‚æ•°åŒºåˆ†

---

## ğŸ’¬ æ€»ç»“

### åŸåˆ†ææ–‡æ¡£çš„ä»·å€¼

âœ… **åšå¾—å¥½çš„ï¼š**
- è¯¦ç»†æ¢³ç†äº†æ‰€æœ‰è„šæœ¬
- è¯†åˆ«äº†ä»£ç é‡å¤é—®é¢˜
- æå‡ºäº†é‡æ„æ€è·¯

âŒ **éœ€è¦æ”¹è¿›çš„ï¼š**
- **è¿‡åº¦è®¾è®¡** - æå‡ºçš„æ–¹æ¡ˆå¤ªå¤æ‚
- **æ²¡æŠ“é‡ç‚¹** - åº”è¯¥ç›´æ¥åˆ é™¤ï¼Œè€Œä¸æ˜¯"æ•´åˆ"
- **å­¦é™¢æ´¾æ€ç»´** - è¿½æ±‚å®Œç¾æ¶æ„ï¼Œå¿½è§†å®ç”¨æ€§

---

### MLå·¥ç¨‹å¸ˆçš„å…³é”®æ´å¯Ÿ

> **"The best code is no code at all."**

1. **åˆ é™¤ > é‡æ„ > ä¼˜åŒ–**
   - 70%çš„è„šæœ¬åº”è¯¥åˆ é™¤
   - ä¸æ˜¯æ•´åˆï¼Œæ˜¯æ¶ˆé™¤

2. **ç®€å• > ä¼˜é›…**
   - å•ä¸€è„šæœ¬ > æ¨¡å—åŒ–æ¡†æ¶
   - ç›´æ¥é€»è¾‘ > æŠ½è±¡è®¾è®¡

3. **é…ç½® > ä»£ç **
   - å‚æ•°å¤–ç½®åˆ°YAML
   - ä¸è¦ä¸ºæ¯ä¸ªåœºæ™¯å†™ä»£ç 

4. **Spark > Multiprocessing**
   - å·²ç»æœ‰Sparkäº†ï¼Œä¸ºä»€ä¹ˆè¿˜è¦è‡ªå·±å†™å¹¶è¡Œï¼Ÿ
   - ç›¸ä¿¡å·¥å…·ï¼Œåˆ«é‡æ–°å‘æ˜è½®å­

---

### è¡ŒåŠ¨å»ºè®®

**æœ¬å‘¨å†…ï¼š**
```bash
# 1. å¤‡ä»½
git commit -am "backup before refactor"

# 2. æ¸…ç†
rm 0ã€*.py æŸ¥çœ‹*.py test_data_process*.py train_data_process.py

# 3. é‡å†™
# åªå†™ä¸€ä¸ª data_process.py (~500è¡Œ)

# 4. æµ‹è¯•
spark-submit data_process.py --config config.yaml
```

**ä¸‹å‘¨ï¼š**
- å…³æ³¨æ¨¡å‹è®­ç»ƒ
- æ•°æ®å¤„ç†è·‘é€šå°±è¡Œï¼Œåˆ«å†æŠ˜è…¾

---

**è®°ä½ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä¸æ˜¯å®Œç¾çš„æ•°æ®å¤„ç†ä»£ç ã€‚**

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** v2.0 (ML Engineering Review)  
**è¯„å®¡äººï¼š** Senior ML Engineer Perspective  
**æ—¥æœŸï¼š** 2025-11-26
