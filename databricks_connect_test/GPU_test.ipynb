{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35690f3d-2e52-40f4-8ffb-438210611ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde929ed-7240-4f83-9274-bd374851cec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. GPU环境验证脚本 (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aed26e9-d981-4f1c-922a-326930820629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NVIDIA驱动信息 ===\nThu Nov  6 06:33:11 2025       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  Tesla V100-PCIE-16GB           Off | 00000001:00:00.0 Off |                  Off |\r\n| N/A   28C    P0              35W / 250W |   3260MiB / 16384MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n\n=== CUDA版本 ===\n/bin/bash: line 1: nvcc: command not found\r\n\n=== Python包检查 ===\ntensorflow: 2.17.0\ntorch: 2.6.0+cu124\npandas: 1.5.3\nnumpy: 1.26.4\n\n=== PyTorch GPU检测 ===\nPyTorch版本: 2.6.0+cu124\nCUDA可用: True\nCUDA版本: 12.4\nGPU数量: 1\nGPU 0: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# GPU环境基础验证\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 检查NVIDIA驱动\n",
    "print(\"=== NVIDIA驱动信息 ===\")\n",
    "!nvidia-smi\n",
    "\n",
    "# 检查CUDA\n",
    "print(\"\\n=== CUDA版本 ===\")\n",
    "!nvcc --version\n",
    "\n",
    "# 检查Python环境\n",
    "print(\"\\n=== Python包检查 ===\")\n",
    "import pkg_resources\n",
    "for package in ['tensorflow', 'torch', 'pandas', 'numpy']:\n",
    "    try:\n",
    "        dist = pkg_resources.get_distribution(package)\n",
    "        print(f\"{package}: {dist.version}\")\n",
    "    except:\n",
    "        print(f\"{package}: 未安装\")\n",
    "\n",
    "\n",
    "# 检查GPU是否被PyTorch识别\n",
    "print(\"\\n=== PyTorch GPU检测 ===\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch版本: {torch.__version__}\")\n",
    "    print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "    print(f\"GPU数量: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch未安装\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2071b13a-2ac3-4ecb-8373-bd18c3c7dae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. GPU性能基准测试脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73aea5d5-bf9b-4912-9041-5c1092ff150f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TensorFlow GPU性能测试 ===\nTensorFlow 10000x10000矩阵乘法: 5.42秒\n\n=== PyTorch GPU性能测试 ===\nPyTorch 10000x10000矩阵乘法(平均): 0.15秒\nGPU显存使用: 1.13 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU性能基准测试\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def gpu_benchmark():\n",
    "    \"\"\"运行简单的GPU性能测试\"\"\"\n",
    "    \n",
    "    # TensorFlow矩阵运算测试\n",
    "    print(\"=== TensorFlow GPU性能测试 ===\")\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # 创建大矩阵\n",
    "        size = 10000\n",
    "        a = tf.random.normal((size, size))\n",
    "        b = tf.random.normal((size, size))\n",
    "        \n",
    "        # GPU矩阵乘法\n",
    "        start_time = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        # 强制计算\n",
    "        _ = c.numpy()\n",
    "        tf_time = time.time() - start_time\n",
    "        print(f\"TensorFlow 10000x10000矩阵乘法: {tf_time:.2f}秒\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"TensorFlow测试失败: {e}\")\n",
    "    \n",
    "    # PyTorch性能测试\n",
    "    print(\"\\n=== PyTorch GPU性能测试 ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        \n",
    "        # 移动到GPU\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        size = 10000\n",
    "        \n",
    "        a = torch.randn(size, size, device=device)\n",
    "        b = torch.randn(size, size, device=device)\n",
    "        \n",
    "        # 预热GPU\n",
    "        for _ in range(10):\n",
    "            _ = torch.mm(a, b)\n",
    "        \n",
    "        torch.cuda.synchronize()  # 等待GPU完成\n",
    "        \n",
    "        # 正式测试\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            c = torch.mm(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        torch_time = (time.time() - start_time) / 10\n",
    "        \n",
    "        print(f\"PyTorch 10000x10000矩阵乘法(平均): {torch_time:.2f}秒\")\n",
    "        \n",
    "        # 显存使用测试\n",
    "        print(f\"GPU显存使用: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PyTorch测试失败: {e}\")\n",
    "\n",
    "# 运行基准测试\n",
    "gpu_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad1b9b5-7474-4075-b0d7-58fa9ff9ea00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch深度学习训练演示 ===\n训练设备: cuda\n生成模拟数据...\n模型参数量: 2,473,610\n开始训练 (100个epochs)...\nEpoch [1/100] - Loss: 2.3040 - Acc: 10.18% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [2/100] - Loss: 2.3030 - Acc: 9.72% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [3/100] - Loss: 2.3022 - Acc: 10.05% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [4/100] - Loss: 2.3022 - Acc: 10.27% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [5/100] - Loss: 2.3022 - Acc: 10.46% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [6/100] - Loss: 2.3024 - Acc: 10.28% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [7/100] - Loss: 2.3026 - Acc: 10.45% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [8/100] - Loss: 2.3024 - Acc: 10.02% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [9/100] - Loss: 2.3023 - Acc: 10.46% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [10/100] - Loss: 2.3023 - Acc: 10.46% - Time: 0.54s - GPU Mem: 0.07GB\nEpoch [11/100] - Loss: 2.3024 - Acc: 10.12% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [12/100] - Loss: 2.3023 - Acc: 10.45% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [13/100] - Loss: 2.3023 - Acc: 10.37% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [14/100] - Loss: 2.3023 - Acc: 10.55% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [15/100] - Loss: 2.3023 - Acc: 10.17% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [16/100] - Loss: 2.3022 - Acc: 10.72% - Time: 0.54s - GPU Mem: 0.07GB\nEpoch [17/100] - Loss: 2.3021 - Acc: 10.64% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [18/100] - Loss: 2.3024 - Acc: 10.50% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [19/100] - Loss: 2.3022 - Acc: 10.50% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [20/100] - Loss: 2.3023 - Acc: 10.15% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [21/100] - Loss: 2.3022 - Acc: 10.64% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [22/100] - Loss: 2.3017 - Acc: 10.28% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [23/100] - Loss: 2.3019 - Acc: 10.80% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [24/100] - Loss: 2.3024 - Acc: 10.17% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [25/100] - Loss: 2.3023 - Acc: 10.48% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [26/100] - Loss: 2.3022 - Acc: 10.55% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [27/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [28/100] - Loss: 2.3022 - Acc: 10.41% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [29/100] - Loss: 2.3024 - Acc: 10.08% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [30/100] - Loss: 2.3021 - Acc: 10.44% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [31/100] - Loss: 2.3022 - Acc: 10.58% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [32/100] - Loss: 2.3020 - Acc: 10.47% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [33/100] - Loss: 2.3021 - Acc: 10.46% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [34/100] - Loss: 2.3022 - Acc: 11.01% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [35/100] - Loss: 2.3021 - Acc: 10.48% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [36/100] - Loss: 2.3022 - Acc: 10.44% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [37/100] - Loss: 2.3022 - Acc: 10.50% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [38/100] - Loss: 2.3022 - Acc: 10.34% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [39/100] - Loss: 2.3022 - Acc: 10.36% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [40/100] - Loss: 2.3019 - Acc: 10.54% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [41/100] - Loss: 2.3023 - Acc: 10.52% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [42/100] - Loss: 2.3020 - Acc: 10.54% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [43/100] - Loss: 2.3023 - Acc: 10.19% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [44/100] - Loss: 2.3020 - Acc: 10.69% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [45/100] - Loss: 2.3023 - Acc: 10.46% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [46/100] - Loss: 2.3020 - Acc: 10.56% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [47/100] - Loss: 2.3021 - Acc: 10.44% - Time: 0.54s - GPU Mem: 0.07GB\nEpoch [48/100] - Loss: 2.3022 - Acc: 10.48% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [49/100] - Loss: 2.3022 - Acc: 10.51% - Time: 0.98s - GPU Mem: 0.07GB\nEpoch [50/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [51/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [52/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [53/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [54/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [55/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [56/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [57/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [58/100] - Loss: 2.3023 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [59/100] - Loss: 2.3020 - Acc: 10.59% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [60/100] - Loss: 2.3021 - Acc: 10.23% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [61/100] - Loss: 2.3023 - Acc: 10.51% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [62/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [63/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [64/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [65/100] - Loss: 2.3023 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [66/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [67/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [68/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [69/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [70/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [71/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [72/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [73/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [74/100] - Loss: 2.3019 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [75/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [76/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [77/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [78/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [79/100] - Loss: 2.3019 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [80/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [81/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [82/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [83/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [84/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [85/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [86/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [87/100] - Loss: 2.3023 - Acc: 10.18% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [88/100] - Loss: 2.3024 - Acc: 10.52% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [89/100] - Loss: 2.3019 - Acc: 10.56% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [90/100] - Loss: 2.3020 - Acc: 10.54% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [91/100] - Loss: 2.3022 - Acc: 10.58% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [92/100] - Loss: 2.3021 - Acc: 10.43% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [93/100] - Loss: 2.3021 - Acc: 10.37% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [94/100] - Loss: 2.3022 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\nEpoch [95/100] - Loss: 2.3022 - Acc: 10.47% - Time: 0.53s - GPU Mem: 0.07GB\nEpoch [96/100] - Loss: 2.3022 - Acc: 10.48% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [97/100] - Loss: 2.3022 - Acc: 10.42% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [98/100] - Loss: 2.3019 - Acc: 10.54% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [99/100] - Loss: 2.3020 - Acc: 10.53% - Time: 0.52s - GPU Mem: 0.07GB\nEpoch [100/100] - Loss: 2.3021 - Acc: 10.53% - Time: 0.51s - GPU Mem: 0.07GB\n\n=== 训练完成 ===\n峰值显存使用: 1.50 GB\n"
     ]
    }
   ],
   "source": [
    "# 深度学习模型训练验证 - PyTorch版本\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"简单的CNN模型用于验证\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def training_demo():\n",
    "    \"\"\"训练演示\"\"\"\n",
    "    print(\"=== PyTorch深度学习训练演示 ===\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"训练设备: {device}\")\n",
    "    \n",
    "    # 创建模拟数据 (CIFAR-10尺寸)\n",
    "    batch_size = 128\n",
    "    num_samples = 10000\n",
    "    num_epochs = 100\n",
    "    \n",
    "    # 生成随机训练数据\n",
    "    print(\"生成模拟数据...\")\n",
    "    x_train = torch.randn(num_samples, 3, 32, 32)\n",
    "    y_train = torch.randint(0, 10, (num_samples,))\n",
    "    \n",
    "    dataset = TensorDataset(x_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"开始训练 ({num_epochs}个epochs)...\")\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "              f'Loss: {running_loss/len(dataloader):.4f} - '\n",
    "              f'Acc: {accuracy:.2f}% - '\n",
    "              f'Time: {epoch_time:.2f}s - '\n",
    "              f'GPU Mem: {torch.cuda.memory_allocated()/1024**3:.2f}GB')\n",
    "    \n",
    "    print(\"\\n=== 训练完成 ===\")\n",
    "    print(f\"峰值显存使用: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "# 运行训练演示\n",
    "training_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87386447-159b-4043-b81c-05357f0678cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Databricks高级深度学习训练演示 ===\n确保目录存在: /mnt/gpu_test\n确保目录存在: /mnt/gpu_test/models\n确保目录存在: /mnt/gpu_test/checkpoints\n确保目录存在: /mnt/gpu_test/logs\n分布式训练已初始化 - Rank: 0, World Size: 1, Local Rank: 0\n训练设备: cuda:0\n检测到 1 个GPU\n有效batch size: 256\n学习率: 0.1\n准备数据加载器...\n初始化模型...\n模型总参数量: 3,250,122\n可训练参数量: 3,250,122\n训练样本数: 50,000\n验证样本数: 10,000\n开始训练 (50个epochs)...\n----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch [0/196], Loss: 2.3107\n  Batch [50/196], Loss: 1.8993\n  Batch [100/196], Loss: 1.7875\n  Batch [150/196], Loss: 1.5688\nEpoch [001/050] | Train Loss: 1.8461 | Val Loss: 1.4535 | Train Acc: 35.27% | Val Acc: 47.38% | LR: 0.100000 | ETA: 9.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 47.38% ***\n  Batch [0/196], Loss: 1.5420\n  Batch [50/196], Loss: 1.3715\n  Batch [100/196], Loss: 1.3884\n  Batch [150/196], Loss: 1.3731\nEpoch [002/050] | Train Loss: 1.3935 | Val Loss: 1.0289 | Train Acc: 50.12% | Val Acc: 63.13% | LR: 0.099901 | ETA: 9.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 63.13% ***\n  Batch [0/196], Loss: 1.2240\n  Batch [50/196], Loss: 1.1372\n  Batch [100/196], Loss: 1.1599\n  Batch [150/196], Loss: 1.1919\nEpoch [003/050] | Train Loss: 1.1492 | Val Loss: 0.8458 | Train Acc: 59.14% | Val Acc: 70.17% | LR: 0.099606 | ETA: 9.0min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 70.17% ***\n  Batch [0/196], Loss: 1.0555\n  Batch [50/196], Loss: 1.0064\n  Batch [100/196], Loss: 1.0192\n  Batch [150/196], Loss: 0.9546\nEpoch [004/050] | Train Loss: 0.9909 | Val Loss: 0.8193 | Train Acc: 65.09% | Val Acc: 72.00% | LR: 0.099114 | ETA: 8.8min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 72.00% ***\n  Batch [0/196], Loss: 0.9178\n  Batch [50/196], Loss: 0.9702\n  Batch [100/196], Loss: 0.8417\n  Batch [150/196], Loss: 0.9308\nEpoch [005/050] | Train Loss: 0.8885 | Val Loss: 0.7353 | Train Acc: 68.69% | Val Acc: 74.72% | LR: 0.098429 | ETA: 8.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 74.72% ***\n  Batch [0/196], Loss: 0.9269\n  Batch [50/196], Loss: 0.8349\n  Batch [100/196], Loss: 0.8997\n  Batch [150/196], Loss: 0.8094\nEpoch [006/050] | Train Loss: 0.8174 | Val Loss: 0.6681 | Train Acc: 71.27% | Val Acc: 77.08% | LR: 0.097553 | ETA: 8.5min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 77.08% ***\n  Batch [0/196], Loss: 0.6573\n  Batch [50/196], Loss: 0.7506\n  Batch [100/196], Loss: 0.9296\n  Batch [150/196], Loss: 0.8328\nEpoch [007/050] | Train Loss: 0.7628 | Val Loss: 0.5909 | Train Acc: 73.46% | Val Acc: 79.68% | LR: 0.096489 | ETA: 8.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 79.68% ***\n  Batch [0/196], Loss: 0.7541\n  Batch [50/196], Loss: 0.7016\n  Batch [100/196], Loss: 0.7070\n  Batch [150/196], Loss: 0.8719\nEpoch [008/050] | Train Loss: 0.7098 | Val Loss: 0.5782 | Train Acc: 75.09% | Val Acc: 80.23% | LR: 0.095241 | ETA: 8.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 80.23% ***\n  Batch [0/196], Loss: 0.7154\n  Batch [50/196], Loss: 0.6926\n  Batch [100/196], Loss: 0.6200\n  Batch [150/196], Loss: 0.7072\nEpoch [009/050] | Train Loss: 0.6728 | Val Loss: 0.5407 | Train Acc: 76.86% | Val Acc: 81.02% | LR: 0.093815 | ETA: 7.9min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 81.02% ***\n  Batch [0/196], Loss: 0.5800\n  Batch [50/196], Loss: 0.7308\n  Batch [100/196], Loss: 0.6457\n  Batch [150/196], Loss: 0.5838\nEpoch [010/050] | Train Loss: 0.6379 | Val Loss: 0.4910 | Train Acc: 77.94% | Val Acc: 83.00% | LR: 0.092216 | ETA: 7.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 83.00% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_10.pth\n  Batch [0/196], Loss: 0.5449\n  Batch [50/196], Loss: 0.5332\n  Batch [100/196], Loss: 0.5353\n  Batch [150/196], Loss: 0.5496\nEpoch [011/050] | Train Loss: 0.5948 | Val Loss: 0.5041 | Train Acc: 79.25% | Val Acc: 83.07% | LR: 0.090451 | ETA: 7.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 83.07% ***\n  Batch [0/196], Loss: 0.5090\n  Batch [50/196], Loss: 0.5028\n  Batch [100/196], Loss: 0.6352\n  Batch [150/196], Loss: 0.6011\nEpoch [012/050] | Train Loss: 0.5734 | Val Loss: 0.4508 | Train Acc: 80.19% | Val Acc: 84.72% | LR: 0.088526 | ETA: 7.4min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 84.72% ***\n  Batch [0/196], Loss: 0.4829\n  Batch [50/196], Loss: 0.5248\n  Batch [100/196], Loss: 0.5381\n  Batch [150/196], Loss: 0.4649\nEpoch [013/050] | Train Loss: 0.5466 | Val Loss: 0.4442 | Train Acc: 81.24% | Val Acc: 85.30% | LR: 0.086448 | ETA: 7.2min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 85.30% ***\n  Batch [0/196], Loss: 0.4284\n  Batch [50/196], Loss: 0.5533\n  Batch [100/196], Loss: 0.4378\n  Batch [150/196], Loss: 0.4489\nEpoch [014/050] | Train Loss: 0.5304 | Val Loss: 0.4399 | Train Acc: 81.68% | Val Acc: 84.85% | LR: 0.084227 | ETA: 7.0min\n  Batch [0/196], Loss: 0.4853\n  Batch [50/196], Loss: 0.4967\n  Batch [100/196], Loss: 0.5198\n  Batch [150/196], Loss: 0.5562\nEpoch [015/050] | Train Loss: 0.5103 | Val Loss: 0.4526 | Train Acc: 82.43% | Val Acc: 84.62% | LR: 0.081871 | ETA: 6.8min\n  Batch [0/196], Loss: 0.5760\n  Batch [50/196], Loss: 0.5720\n  Batch [100/196], Loss: 0.4282\n  Batch [150/196], Loss: 0.5254\nEpoch [016/050] | Train Loss: 0.4916 | Val Loss: 0.4208 | Train Acc: 83.07% | Val Acc: 85.71% | LR: 0.079389 | ETA: 6.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 85.71% ***\n  Batch [0/196], Loss: 0.4682\n  Batch [50/196], Loss: 0.4981\n  Batch [100/196], Loss: 0.4616\n  Batch [150/196], Loss: 0.4759\nEpoch [017/050] | Train Loss: 0.4766 | Val Loss: 0.4063 | Train Acc: 83.44% | Val Acc: 86.50% | LR: 0.076791 | ETA: 6.4min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 86.50% ***\n  Batch [0/196], Loss: 0.3769\n  Batch [50/196], Loss: 0.4373\n  Batch [100/196], Loss: 0.4802\n  Batch [150/196], Loss: 0.4317\nEpoch [018/050] | Train Loss: 0.4631 | Val Loss: 0.3982 | Train Acc: 83.89% | Val Acc: 86.58% | LR: 0.074088 | ETA: 6.2min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 86.58% ***\n  Batch [0/196], Loss: 0.5096\n  Batch [50/196], Loss: 0.4304\n  Batch [100/196], Loss: 0.3649\n  Batch [150/196], Loss: 0.4557\nEpoch [019/050] | Train Loss: 0.4442 | Val Loss: 0.3911 | Train Acc: 84.72% | Val Acc: 86.77% | LR: 0.071289 | ETA: 6.0min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 86.77% ***\n  Batch [0/196], Loss: 0.4142\n  Batch [50/196], Loss: 0.4345\n  Batch [100/196], Loss: 0.4144\n  Batch [150/196], Loss: 0.4452\nEpoch [020/050] | Train Loss: 0.4273 | Val Loss: 0.3710 | Train Acc: 85.05% | Val Acc: 87.29% | LR: 0.068406 | ETA: 5.8min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 87.29% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_20.pth\n  Batch [0/196], Loss: 0.3705\n  Batch [50/196], Loss: 0.4244\n  Batch [100/196], Loss: 0.4429\n  Batch [150/196], Loss: 0.4596\nEpoch [021/050] | Train Loss: 0.4205 | Val Loss: 0.3434 | Train Acc: 85.45% | Val Acc: 88.07% | LR: 0.065451 | ETA: 5.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.07% ***\n  Batch [0/196], Loss: 0.3704\n  Batch [50/196], Loss: 0.4304\n  Batch [100/196], Loss: 0.4388\n  Batch [150/196], Loss: 0.4205\nEpoch [022/050] | Train Loss: 0.4070 | Val Loss: 0.3528 | Train Acc: 85.91% | Val Acc: 88.16% | LR: 0.062434 | ETA: 5.4min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.16% ***\n  Batch [0/196], Loss: 0.4065\n  Batch [50/196], Loss: 0.3731\n  Batch [100/196], Loss: 0.3499\n  Batch [150/196], Loss: 0.3568\nEpoch [023/050] | Train Loss: 0.3921 | Val Loss: 0.3594 | Train Acc: 86.53% | Val Acc: 88.14% | LR: 0.059369 | ETA: 5.3min\n  Batch [0/196], Loss: 0.3268\n  Batch [50/196], Loss: 0.3215\n  Batch [100/196], Loss: 0.3580\n  Batch [150/196], Loss: 0.3974\nEpoch [024/050] | Train Loss: 0.3814 | Val Loss: 0.3494 | Train Acc: 86.85% | Val Acc: 88.29% | LR: 0.056267 | ETA: 5.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.29% ***\n  Batch [0/196], Loss: 0.4071\n  Batch [50/196], Loss: 0.3739\n  Batch [100/196], Loss: 0.4085\n  Batch [150/196], Loss: 0.4187\nEpoch [025/050] | Train Loss: 0.3659 | Val Loss: 0.3489 | Train Acc: 87.21% | Val Acc: 88.65% | LR: 0.053140 | ETA: 4.9min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.65% ***\n  Batch [0/196], Loss: 0.2687\n  Batch [50/196], Loss: 0.3752\n  Batch [100/196], Loss: 0.3754\n  Batch [150/196], Loss: 0.4139\nEpoch [026/050] | Train Loss: 0.3597 | Val Loss: 0.3358 | Train Acc: 87.47% | Val Acc: 88.68% | LR: 0.050000 | ETA: 4.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.68% ***\n  Batch [0/196], Loss: 0.3169\n  Batch [50/196], Loss: 0.3401\n  Batch [100/196], Loss: 0.4195\n  Batch [150/196], Loss: 0.4058\nEpoch [027/050] | Train Loss: 0.3450 | Val Loss: 0.3385 | Train Acc: 87.90% | Val Acc: 88.47% | LR: 0.046860 | ETA: 4.5min\n  Batch [0/196], Loss: 0.2922\n  Batch [50/196], Loss: 0.2977\n  Batch [100/196], Loss: 0.3204\n  Batch [150/196], Loss: 0.2925\nEpoch [028/050] | Train Loss: 0.3418 | Val Loss: 0.3288 | Train Acc: 87.97% | Val Acc: 89.21% | LR: 0.043733 | ETA: 4.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.21% ***\n  Batch [0/196], Loss: 0.2681\n  Batch [50/196], Loss: 0.3634\n  Batch [100/196], Loss: 0.4127\n  Batch [150/196], Loss: 0.3469\nEpoch [029/050] | Train Loss: 0.3235 | Val Loss: 0.3281 | Train Acc: 88.74% | Val Acc: 88.77% | LR: 0.040631 | ETA: 4.1min\n  Batch [0/196], Loss: 0.3392\n  Batch [50/196], Loss: 0.2794\n  Batch [100/196], Loss: 0.2732\n  Batch [150/196], Loss: 0.2056\nEpoch [030/050] | Train Loss: 0.3208 | Val Loss: 0.3144 | Train Acc: 88.81% | Val Acc: 89.42% | LR: 0.037566 | ETA: 3.9min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.42% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_30.pth\n  Batch [0/196], Loss: 0.3509\n  Batch [50/196], Loss: 0.3034\n  Batch [100/196], Loss: 0.2709\n  Batch [150/196], Loss: 0.2765\nEpoch [031/050] | Train Loss: 0.3075 | Val Loss: 0.3174 | Train Acc: 89.39% | Val Acc: 89.37% | LR: 0.034549 | ETA: 3.7min\n  Batch [0/196], Loss: 0.3254\n  Batch [50/196], Loss: 0.3352\n  Batch [100/196], Loss: 0.4085\n  Batch [150/196], Loss: 0.2620\nEpoch [032/050] | Train Loss: 0.2983 | Val Loss: 0.3062 | Train Acc: 89.53% | Val Acc: 89.80% | LR: 0.031594 | ETA: 3.5min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.80% ***\n  Batch [0/196], Loss: 0.2908\n  Batch [50/196], Loss: 0.3885\n  Batch [100/196], Loss: 0.4150\n  Batch [150/196], Loss: 0.2828\nEpoch [033/050] | Train Loss: 0.2900 | Val Loss: 0.3138 | Train Acc: 89.91% | Val Acc: 89.88% | LR: 0.028711 | ETA: 3.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.88% ***\n  Batch [0/196], Loss: 0.2403\n  Batch [50/196], Loss: 0.3636\n  Batch [100/196], Loss: 0.2401\n  Batch [150/196], Loss: 0.2502\nEpoch [034/050] | Train Loss: 0.2801 | Val Loss: 0.3052 | Train Acc: 90.30% | Val Acc: 90.09% | LR: 0.025912 | ETA: 3.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.09% ***\n  Batch [0/196], Loss: 0.2281\n  Batch [50/196], Loss: 0.3043\n  Batch [100/196], Loss: 0.3636\n  Batch [150/196], Loss: 0.2627\nEpoch [035/050] | Train Loss: 0.2751 | Val Loss: 0.3132 | Train Acc: 90.53% | Val Acc: 89.92% | LR: 0.023209 | ETA: 2.9min\n  Batch [0/196], Loss: 0.2956\n  Batch [50/196], Loss: 0.2549\n  Batch [100/196], Loss: 0.3135\n  Batch [150/196], Loss: 0.3470\nEpoch [036/050] | Train Loss: 0.2681 | Val Loss: 0.2948 | Train Acc: 90.75% | Val Acc: 90.33% | LR: 0.020611 | ETA: 2.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.33% ***\n  Batch [0/196], Loss: 0.2072\n  Batch [50/196], Loss: 0.3050\n  Batch [100/196], Loss: 0.3667\n  Batch [150/196], Loss: 0.2890\nEpoch [037/050] | Train Loss: 0.2568 | Val Loss: 0.2925 | Train Acc: 91.07% | Val Acc: 90.46% | LR: 0.018129 | ETA: 2.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.46% ***\n  Batch [0/196], Loss: 0.2835\n  Batch [50/196], Loss: 0.1914\n  Batch [100/196], Loss: 0.2085\n  Batch [150/196], Loss: 0.2465\nEpoch [038/050] | Train Loss: 0.2513 | Val Loss: 0.2984 | Train Acc: 91.22% | Val Acc: 90.31% | LR: 0.015773 | ETA: 2.4min\n  Batch [0/196], Loss: 0.2589\n  Batch [50/196], Loss: 0.2839\n  Batch [100/196], Loss: 0.2658\n  Batch [150/196], Loss: 0.2544\nEpoch [039/050] | Train Loss: 0.2409 | Val Loss: 0.2897 | Train Acc: 91.57% | Val Acc: 90.88% | LR: 0.013552 | ETA: 2.2min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.88% ***\n  Batch [0/196], Loss: 0.2619\n  Batch [50/196], Loss: 0.2656\n  Batch [100/196], Loss: 0.2339\n  Batch [150/196], Loss: 0.1819\nEpoch [040/050] | Train Loss: 0.2381 | Val Loss: 0.2811 | Train Acc: 91.67% | Val Acc: 90.99% | LR: 0.011474 | ETA: 2.0min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.99% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_40.pth\n  Batch [0/196], Loss: 0.1832\n  Batch [50/196], Loss: 0.1792\n  Batch [100/196], Loss: 0.2167\n  Batch [150/196], Loss: 0.2380\nEpoch [041/050] | Train Loss: 0.2304 | Val Loss: 0.2823 | Train Acc: 92.05% | Val Acc: 91.05% | LR: 0.009549 | ETA: 1.8min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.05% ***\n  Batch [0/196], Loss: 0.2325\n  Batch [50/196], Loss: 0.2217\n  Batch [100/196], Loss: 0.2570\n  Batch [150/196], Loss: 0.1928\nEpoch [042/050] | Train Loss: 0.2294 | Val Loss: 0.2847 | Train Acc: 92.06% | Val Acc: 90.88% | LR: 0.007784 | ETA: 1.6min\n  Batch [0/196], Loss: 0.2136\n  Batch [50/196], Loss: 0.2599\n  Batch [100/196], Loss: 0.2570\n  Batch [150/196], Loss: 0.2712\nEpoch [043/050] | Train Loss: 0.2241 | Val Loss: 0.2812 | Train Acc: 92.31% | Val Acc: 90.90% | LR: 0.006185 | ETA: 1.4min\n  Batch [0/196], Loss: 0.2189\n  Batch [50/196], Loss: 0.1872\n  Batch [100/196], Loss: 0.2478\n  Batch [150/196], Loss: 0.1913\nEpoch [044/050] | Train Loss: 0.2230 | Val Loss: 0.2784 | Train Acc: 92.14% | Val Acc: 91.14% | LR: 0.004759 | ETA: 1.2min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.14% ***\n  Batch [0/196], Loss: 0.2610\n  Batch [50/196], Loss: 0.1884\n  Batch [100/196], Loss: 0.2048\n  Batch [150/196], Loss: 0.2013\nEpoch [045/050] | Train Loss: 0.2169 | Val Loss: 0.2736 | Train Acc: 92.48% | Val Acc: 91.23% | LR: 0.003511 | ETA: 1.0min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.23% ***\n  Batch [0/196], Loss: 0.1808\n  Batch [50/196], Loss: 0.2069\n  Batch [100/196], Loss: 0.2364\n  Batch [150/196], Loss: 0.1965\nEpoch [046/050] | Train Loss: 0.2153 | Val Loss: 0.2767 | Train Acc: 92.43% | Val Acc: 91.19% | LR: 0.002447 | ETA: 0.8min\n  Batch [0/196], Loss: 0.2386\n  Batch [50/196], Loss: 0.2856\n  Batch [100/196], Loss: 0.2030\n  Batch [150/196], Loss: 0.2474\nEpoch [047/050] | Train Loss: 0.2157 | Val Loss: 0.2740 | Train Acc: 92.44% | Val Acc: 91.32% | LR: 0.001571 | ETA: 0.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.32% ***\n  Batch [0/196], Loss: 0.2599\n  Batch [50/196], Loss: 0.2320\n  Batch [100/196], Loss: 0.2089\n  Batch [150/196], Loss: 0.2062\nEpoch [048/050] | Train Loss: 0.2119 | Val Loss: 0.2755 | Train Acc: 92.62% | Val Acc: 91.25% | LR: 0.000886 | ETA: 0.4min\n  Batch [0/196], Loss: 0.1561\n  Batch [50/196], Loss: 0.2386\n  Batch [100/196], Loss: 0.2112\n  Batch [150/196], Loss: 0.2295\nEpoch [049/050] | Train Loss: 0.2117 | Val Loss: 0.2747 | Train Acc: 92.64% | Val Acc: 91.31% | LR: 0.000394 | ETA: 0.2min\n  Batch [0/196], Loss: 0.2542\n  Batch [50/196], Loss: 0.1596\n  Batch [100/196], Loss: 0.2358\n  Batch [150/196], Loss: 0.1723\nEpoch [050/050] | Train Loss: 0.2126 | Val Loss: 0.2793 | Train Acc: 92.62% | Val Acc: 91.10% | LR: 0.000099 | ETA: 0.0min\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_50.pth\n分布式训练资源已清理\n\n================================================================================\n=== 训练完成 ===\n最佳验证准确率: 91.32%\n峰值显存使用: 1.50 GB\n1106-055846-je18h7lt-172-21-8-132:1737:1737 [0] NCCL INFO comm 0x46bdda40 rank 0 nranks 1 cudaDev 0 busId 100000 - Destroy COMPLETE\n模型已保存到: /mnt/gpu_test/models/final_model.pth\n训练统计图已保存到: /mnt/gpu_test/logs/training_stats.png\n所有文件保存在: /mnt/gpu_test\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 创建必要的目录\n",
    "def setup_directories():\n",
    "    \"\"\"创建模型保存目录\"\"\"\n",
    "    base_path = \"/mnt/gpu_test\"\n",
    "    directories = [\n",
    "        base_path,\n",
    "        f\"{base_path}/models\",\n",
    "        f\"{base_path}/checkpoints\",\n",
    "        f\"{base_path}/logs\"\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            print(f\"确保目录存在: {directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"创建目录 {directory} 时出错: {e}\")\n",
    "    \n",
    "    return base_path\n",
    "\n",
    "class AdvancedCNN(nn.Module):\n",
    "    \"\"\"改进的CNN模型，使用更现代的结构\"\"\"\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取层 - 使用更深的网络\n",
    "        self.features = nn.Sequential(\n",
    "            # 第一组卷积层\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(dropout_rate/2),\n",
    "            \n",
    "            # 第二组卷积层\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # 第三组卷积层\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"更好的权重初始化\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    \"\"\"CIFAR-10数据集包装器，支持数据增强\"\"\"\n",
    "    def __init__(self, train=True, augment=True):\n",
    "        self.train = train\n",
    "        self.augment = augment\n",
    "        \n",
    "        # 定义数据变换\n",
    "        if self.train and self.augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "        \n",
    "        # 在Databricks中，您可能需要从DBFS加载数据\n",
    "        # 这里使用torchvision的CIFAR-10作为示例\n",
    "        self.dataset = datasets.CIFAR10(\n",
    "            root='/tmp/cifar10', \n",
    "            train=self.train, \n",
    "            download=True, \n",
    "            transform=self.transform\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"训练监控器\"\"\"\n",
    "    def __init__(self, base_path):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_accs = []\n",
    "        self.lr_history = []\n",
    "        self.start_time = time.time()\n",
    "        self.base_path = base_path\n",
    "        \n",
    "        # 保存训练日志\n",
    "        self.log_file = f\"{base_path}/logs/training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write(\"Epoch,Train_Loss,Val_Loss,Train_Acc,Val_Acc,Learning_Rate\\n\")\n",
    "    \n",
    "    def update(self, epoch, train_loss, val_loss, train_acc, val_acc, lr):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_accs.append(train_acc)\n",
    "        self.val_accs.append(val_acc)\n",
    "        self.lr_history.append(lr)\n",
    "        \n",
    "        # 记录到文件\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(f\"{epoch+1},{train_loss:.6f},{val_loss:.6f},{train_acc:.4f},{val_acc:.4f},{lr:.8f}\\n\")\n",
    "    \n",
    "    def print_epoch_stats(self, epoch, total_epochs):\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.start_time\n",
    "        eta = elapsed / (epoch + 1) * (total_epochs - epoch - 1)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1:03d}/{total_epochs:03d}] | '\n",
    "              f'Train Loss: {self.train_losses[-1]:.4f} | '\n",
    "              f'Val Loss: {self.val_losses[-1]:.4f} | '\n",
    "              f'Train Acc: {self.train_accs[-1]:.2f}% | '\n",
    "              f'Val Acc: {self.val_accs[-1]:.2f}% | '\n",
    "              f'LR: {self.lr_history[-1]:.6f} | '\n",
    "              f'ETA: {eta/60:.1f}min')\n",
    "\n",
    "def setup_distributed_training():\n",
    "    \"\"\"设置分布式训练 - 修复重复初始化问题\"\"\"\n",
    "    try:\n",
    "        # 检查是否已经在分布式环境中\n",
    "        if dist.is_initialized():\n",
    "            rank = dist.get_rank()\n",
    "            world_size = dist.get_world_size()\n",
    "            local_rank = rank % torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "            print(f\"分布式训练已初始化 - Rank: {rank}, World Size: {world_size}, Local Rank: {local_rank}\")\n",
    "            return True, rank, world_size, local_rank\n",
    "        \n",
    "        # 检查环境变量\n",
    "        if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "            rank = int(os.environ['RANK'])\n",
    "            world_size = int(os.environ['WORLD_SIZE'])\n",
    "            local_rank = int(os.environ.get('LOCAL_RANK', rank % torch.cuda.device_count()))\n",
    "            \n",
    "            # 初始化分布式训练\n",
    "            dist.init_process_group(backend='nccl', init_method='env://')\n",
    "            torch.cuda.set_device(local_rank)\n",
    "            \n",
    "            print(f\"成功初始化分布式训练 - Rank: {rank}, World Size: {world_size}, Local Rank: {local_rank}\")\n",
    "            return True, rank, world_size, local_rank\n",
    "        \n",
    "        # 单机训练\n",
    "        print(\"使用单机训练模式\")\n",
    "        return False, 0, 1, 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"分布式训练设置失败: {e}，使用单机模式\")\n",
    "        return False, 0, 1, 0\n",
    "\n",
    "def cleanup_distributed():\n",
    "    \"\"\"清理分布式训练资源\"\"\"\n",
    "    try:\n",
    "        if dist.is_initialized():\n",
    "            dist.destroy_process_group()\n",
    "            print(\"分布式训练资源已清理\")\n",
    "    except Exception as e:\n",
    "        print(f\"清理分布式训练资源时出错: {e}\")\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, scheduler_step=None):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # 每50个batch打印一次进度\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'  Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # 动态学习率调整（如果使用step调度器）\n",
    "        if scheduler_step is not None:\n",
    "            scheduler_step.step()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"验证模型\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def save_checkpoint(state, filename, base_path):\n",
    "    \"\"\"保存检查点，确保目录存在\"\"\"\n",
    "    full_path = f\"{base_path}/{filename}\"\n",
    "    directory = os.path.dirname(full_path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    torch.save(state, full_path)\n",
    "    print(f\"模型已保存到: {full_path}\")\n",
    "\n",
    "def advanced_training_demo():\n",
    "    \"\"\"高级训练演示 - 专为Databricks GPU集群设计\"\"\"\n",
    "    print(\"=== Databricks高级深度学习训练演示 ===\")\n",
    "    \n",
    "    # 设置目录\n",
    "    base_path = setup_directories()\n",
    "    \n",
    "    # 设置分布式训练\n",
    "    is_distributed, rank, world_size, local_rank = setup_distributed_training()\n",
    "    device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"训练设备: {device}\")\n",
    "    \n",
    "    # 超参数配置 - 根据GPU数量调整\n",
    "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    effective_batch_size = 256 * num_gpus  # 根据GPU数量缩放batch size\n",
    "    \n",
    "    config = {\n",
    "        'batch_size': effective_batch_size,\n",
    "        'num_epochs': 50,  # 减少epoch数用于演示\n",
    "        'learning_rate': 0.1 * num_gpus,  # 根据GPU数量调整学习率\n",
    "        'weight_decay': 1e-4,\n",
    "        'momentum': 0.9,\n",
    "        'dropout_rate': 0.3,\n",
    "        'num_workers': 4,\n",
    "        'pin_memory': True,\n",
    "        'num_gpus': num_gpus\n",
    "    }\n",
    "    \n",
    "    # 只在主进程中显示配置信息\n",
    "    if not is_distributed or rank == 0:\n",
    "        print(f\"检测到 {num_gpus} 个GPU\")\n",
    "        print(f\"有效batch size: {config['batch_size']}\")\n",
    "        print(f\"学习率: {config['learning_rate']}\")\n",
    "    \n",
    "    # 数据加载\n",
    "    if not is_distributed or rank == 0:\n",
    "        print(\"准备数据加载器...\")\n",
    "    \n",
    "    train_dataset = CIFAR10Dataset(train=True, augment=True)\n",
    "    val_dataset = CIFAR10Dataset(train=False, augment=False)\n",
    "    \n",
    "    if is_distributed:\n",
    "        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "        val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        val_sampler = None\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'] // world_size if is_distributed else config['batch_size'],\n",
    "        shuffle=(train_sampler is None),\n",
    "        sampler=train_sampler,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory']\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'] // world_size if is_distributed else config['batch_size'],\n",
    "        shuffle=False,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory']\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    if not is_distributed or rank == 0:\n",
    "        print(\"初始化模型...\")\n",
    "    \n",
    "    model = AdvancedCNN(num_classes=10, dropout_rate=config['dropout_rate'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 分布式数据并行\n",
    "    if is_distributed:\n",
    "        model = DDP(model, device_ids=[local_rank])\n",
    "    \n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        momentum=config['momentum'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler_cosine = CosineAnnealingLR(optimizer, T_max=config['num_epochs'])\n",
    "    scheduler_reduce = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # 训练监控（只在主进程）\n",
    "    monitor = TrainingMonitor(base_path) if (not is_distributed or rank == 0) else None\n",
    "    \n",
    "    # 打印配置信息\n",
    "    if not is_distributed or rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"模型总参数量: {total_params:,}\")\n",
    "        print(f\"可训练参数量: {trainable_params:,}\")\n",
    "        print(f\"训练样本数: {len(train_dataset):,}\")\n",
    "        print(f\"验证样本数: {len(val_dataset):,}\")\n",
    "        print(f\"开始训练 ({config['num_epochs']}个epochs)...\")\n",
    "        print(\"-\" * 100)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        if is_distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        # 训练\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 学习率调度\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler_cosine.step()\n",
    "        scheduler_reduce.step(val_loss)\n",
    "        \n",
    "        # 只在主进程中更新监控器和保存模型\n",
    "        if not is_distributed or rank == 0:\n",
    "            if monitor is not None:\n",
    "                monitor.update(epoch, train_loss, val_loss, train_acc, val_acc, current_lr)\n",
    "                monitor.print_epoch_stats(epoch, config['num_epochs'])\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                # 获取模型状态（如果是DDP，需要获取module）\n",
    "                model_state_dict = model.module.state_dict() if is_distributed else model.state_dict()\n",
    "                \n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model_state_dict,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'config': config\n",
    "                }\n",
    "                save_checkpoint(checkpoint, 'models/best_model.pth', base_path)\n",
    "                print(f\"*** 新的最佳模型保存! 验证准确率: {val_acc:.2f}% ***\")\n",
    "            \n",
    "            # 每10个epoch保存一次检查点\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model_state_dict = model.module.state_dict() if is_distributed else model.state_dict()\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model_state_dict,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_cosine': scheduler_cosine.state_dict(),\n",
    "                    'scheduler_reduce': scheduler_reduce.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'config': config\n",
    "                }\n",
    "                if monitor is not None:\n",
    "                    checkpoint['training_history'] = {\n",
    "                        'train_losses': monitor.train_losses,\n",
    "                        'val_losses': monitor.val_losses,\n",
    "                        'train_accs': monitor.train_accs,\n",
    "                        'val_accs': monitor.val_accs\n",
    "                    }\n",
    "                save_checkpoint(checkpoint, f'checkpoints/checkpoint_epoch_{epoch+1}.pth', base_path)\n",
    "    \n",
    "    # 清理\n",
    "    cleanup_distributed()\n",
    "    \n",
    "    # 最终统计（只在主进程）\n",
    "    if not is_distributed or rank == 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"=== 训练完成 ===\")\n",
    "        print(f\"最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        # GPU内存使用统计\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"峰值显存使用: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB\")\n",
    "        \n",
    "        # 保存最终模型\n",
    "        model_state_dict = model.module.state_dict() if is_distributed else model.state_dict()\n",
    "        final_checkpoint = {\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': best_val_acc,\n",
    "            'config': config\n",
    "        }\n",
    "        if monitor is not None:\n",
    "            final_checkpoint['training_history'] = {\n",
    "                'train_losses': monitor.train_losses,\n",
    "                'val_losses': monitor.val_losses,\n",
    "                'train_accs': monitor.train_accs,\n",
    "                'val_accs': monitor.val_accs\n",
    "            }\n",
    "        save_checkpoint(final_checkpoint, 'models/final_model.pth', base_path)\n",
    "        \n",
    "        # 保存训练统计图\n",
    "        if monitor is not None:\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                \n",
    "                plt.figure(figsize=(15, 5))\n",
    "                \n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.plot(monitor.train_losses, label='Train Loss')\n",
    "                plt.plot(monitor.val_losses, label='Val Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.title('Training and Validation Loss')\n",
    "                \n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.plot(monitor.train_accs, label='Train Acc')\n",
    "                plt.plot(monitor.val_accs, label='Val Acc')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Accuracy (%)')\n",
    "                plt.legend()\n",
    "                plt.title('Training and Validation Accuracy')\n",
    "                \n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.plot(monitor.lr_history)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title('Learning Rate Schedule')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{base_path}/logs/training_stats.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"训练统计图已保存到: {base_path}/logs/training_stats.png\")\n",
    "            except ImportError:\n",
    "                print(\"Matplotlib未安装，跳过绘图\")\n",
    "        \n",
    "        print(f\"所有文件保存在: {base_path}\")\n",
    "\n",
    "# 运行训练演示\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        advanced_training_demo()\n",
    "    except Exception as e:\n",
    "        print(f\"训练过程中出现错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c02ed83d-34a3-4143-b329-9a4da2ff4f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 手动autoscale 到2个gpu后但效率没有提高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b0dbef-da2d-45dc-90d4-b5ceab2cccee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到 2 个GPU:\n  GPU 0: Tesla V100-PCIE-16GB (15.8 GB)\n  GPU 1: Tesla V100-PCIE-16GB (15.8 GB)\n\n总GPU数量: 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "\n",
    "def get_gpu_count():\n",
    "    \"\"\"获取GPU数量\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"检测到 {gpu_count} 个GPU:\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            memory = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB\n",
    "            print(f\"  GPU {i}: {gpu_name} ({memory:.1f} GB)\")\n",
    "        \n",
    "        return gpu_count\n",
    "    else:\n",
    "        print(\"未检测到GPU\")\n",
    "        return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count = get_gpu_count()\n",
    "    print(f\"\\n总GPU数量: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3fba60-8f74-40a5-8b72-f82349224b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Databricks高级深度学习训练演示 ===\n确保目录存在: /mnt/gpu_test\n确保目录存在: /mnt/gpu_test/models\n确保目录存在: /mnt/gpu_test/checkpoints\n确保目录存在: /mnt/gpu_test/logs\n成功初始化分布式训练 - Rank: 0, World Size: 1, Local Rank: 0\n训练设备: cuda:0\n检测到 2 个GPU\n有效batch size: 512\n学习率: 0.2\n准备数据加载器...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0.00/170M [00:00<?, ?B/s]\r  0%|          | 32.8k/170M [00:00<34:12, 83.1kB/s]\r  0%|          | 98.3k/170M [00:00<15:17, 186kB/s] \r  0%|          | 229k/170M [00:00<07:51, 361kB/s] \r  0%|          | 459k/170M [00:01<04:37, 612kB/s]\r  1%|          | 918k/170M [00:01<02:26, 1.16MB/s]\r  1%|          | 1.41M/170M [00:01<01:47, 1.57MB/s]\r  1%|          | 1.93M/170M [00:01<01:28, 1.90MB/s]\r  1%|▏         | 2.46M/170M [00:01<01:23, 2.00MB/s]\r  2%|▏         | 3.05M/170M [00:02<01:13, 2.27MB/s]\r  2%|▏         | 3.64M/170M [00:02<01:07, 2.48MB/s]\r  3%|▎         | 4.29M/170M [00:02<01:01, 2.69MB/s]\r  3%|▎         | 4.95M/170M [00:02<01:01, 2.69MB/s]\r  3%|▎         | 5.64M/170M [00:02<00:56, 2.90MB/s]\r  4%|▎         | 6.39M/170M [00:03<00:52, 3.14MB/s]\r  4%|▍         | 7.14M/170M [00:03<00:49, 3.33MB/s]\r  5%|▍         | 7.96M/170M [00:03<00:47, 3.41MB/s]\r  5%|▌         | 8.81M/170M [00:03<00:44, 3.65MB/s]\r  6%|▌         | 9.70M/170M [00:03<00:41, 3.88MB/s]\r  6%|▌         | 10.6M/170M [00:04<00:38, 4.13MB/s]\r  7%|▋         | 11.6M/170M [00:04<00:37, 4.22MB/s]\r  7%|▋         | 12.6M/170M [00:04<00:34, 4.62MB/s]\r  8%|▊         | 13.7M/170M [00:04<00:32, 4.86MB/s]\r  9%|▊         | 14.9M/170M [00:04<00:30, 5.11MB/s]\r  9%|▉         | 16.1M/170M [00:05<00:28, 5.36MB/s]\r 10%|▉         | 16.6M/170M [00:05<00:30, 5.09MB/s]\r 10%|█         | 17.4M/170M [00:05<00:33, 4.62MB/s]\r 11%|█         | 18.6M/170M [00:05<00:25, 6.06MB/s]\r 11%|█▏        | 19.3M/170M [00:05<00:24, 6.16MB/s]\r 12%|█▏        | 20.1M/170M [00:05<00:27, 5.38MB/s]\r 13%|█▎        | 21.5M/170M [00:05<00:21, 7.09MB/s]\r 13%|█▎        | 22.3M/170M [00:06<00:20, 7.18MB/s]\r 14%|█▎        | 23.1M/170M [00:06<00:24, 6.14MB/s]\r 14%|█▍        | 24.6M/170M [00:06<00:19, 7.49MB/s]\r 15%|█▌        | 26.1M/170M [00:06<00:15, 9.17MB/s]\r 16%|█▌        | 27.1M/170M [00:06<00:17, 8.00MB/s]\r 16%|█▋        | 28.0M/170M [00:06<00:18, 7.74MB/s]\r 17%|█▋        | 29.7M/170M [00:06<00:14, 9.85MB/s]\r 18%|█▊        | 30.8M/170M [00:07<00:16, 8.57MB/s]\r 19%|█▊        | 31.8M/170M [00:07<00:18, 7.64MB/s]\r 20%|█▉        | 33.8M/170M [00:07<00:14, 9.74MB/s]\r 20%|██        | 34.9M/170M [00:07<00:13, 9.97MB/s]\r 21%|██        | 35.9M/170M [00:07<00:14, 9.42MB/s]\r 22%|██▏       | 37.3M/170M [00:07<00:13, 9.90MB/s]\r 22%|██▏       | 38.3M/170M [00:07<00:14, 8.83MB/s]\r 24%|██▍       | 40.5M/170M [00:08<00:11, 11.3MB/s]\r 25%|██▍       | 41.8M/170M [00:08<00:12, 10.6MB/s]\r 25%|██▌       | 43.1M/170M [00:08<00:14, 8.98MB/s]\r 26%|██▌       | 44.1M/170M [00:08<00:15, 7.97MB/s]\r 27%|██▋       | 46.2M/170M [00:08<00:11, 10.6MB/s]\r 28%|██▊       | 47.4M/170M [00:09<00:38, 3.23MB/s]\r 28%|██▊       | 48.3M/170M [00:10<00:51, 2.38MB/s]\r 31%|███       | 52.0M/170M [00:10<00:24, 4.86MB/s]\r 31%|███▏      | 53.5M/170M [00:10<00:21, 5.51MB/s]\r 32%|███▏      | 55.0M/170M [00:10<00:18, 6.37MB/s]\r 34%|███▎      | 57.4M/170M [00:11<00:12, 8.90MB/s]\r 35%|███▍      | 59.0M/170M [00:11<00:11, 9.67MB/s]\r 36%|███▌      | 61.0M/170M [00:11<00:11, 9.54MB/s]\r 38%|███▊      | 64.1M/170M [00:11<00:09, 11.2MB/s]\r 39%|███▊      | 66.1M/170M [00:11<00:08, 12.6MB/s]\r 40%|███▉      | 67.6M/170M [00:11<00:07, 13.0MB/s]\r 41%|████      | 69.5M/170M [00:11<00:07, 14.2MB/s]\r 42%|████▏     | 71.1M/170M [00:12<00:07, 14.1MB/s]\r 43%|████▎     | 73.3M/170M [00:12<00:07, 13.0MB/s]\r 44%|████▍     | 75.8M/170M [00:12<00:06, 15.5MB/s]\r 45%|████▌     | 77.5M/170M [00:12<00:06, 13.7MB/s]\r 47%|████▋     | 79.4M/170M [00:12<00:07, 12.4MB/s]\r 48%|████▊     | 82.5M/170M [00:12<00:06, 13.2MB/s]\r 49%|████▉     | 84.3M/170M [00:12<00:06, 13.8MB/s]\r 50%|█████     | 85.8M/170M [00:13<00:06, 13.2MB/s]\r 51%|█████     | 87.2M/170M [00:13<00:06, 12.9MB/s]\r 52%|█████▏    | 88.5M/170M [00:13<00:06, 12.7MB/s]\r 53%|█████▎    | 89.9M/170M [00:13<00:06, 12.9MB/s]\r 54%|█████▎    | 91.3M/170M [00:13<00:06, 13.1MB/s]\r 54%|█████▍    | 92.7M/170M [00:13<00:05, 13.4MB/s]\r 55%|█████▌    | 94.1M/170M [00:13<00:05, 13.2MB/s]\r 56%|█████▌    | 95.5M/170M [00:13<00:05, 13.1MB/s]\r 57%|█████▋    | 96.8M/170M [00:13<00:06, 11.9MB/s]\r 58%|█████▊    | 98.5M/170M [00:14<00:05, 13.1MB/s]\r 59%|█████▊    | 99.8M/170M [00:14<00:05, 13.1MB/s]\r 59%|█████▉    | 101M/170M [00:14<00:05, 13.1MB/s] \r 60%|██████    | 103M/170M [00:14<00:05, 13.3MB/s]\r 61%|██████    | 104M/170M [00:14<00:05, 11.4MB/s]\r 62%|██████▏   | 106M/170M [00:14<00:05, 12.1MB/s]\r 63%|██████▎   | 107M/170M [00:14<00:05, 11.2MB/s]\r 64%|██████▎   | 109M/170M [00:14<00:04, 12.9MB/s]\r 64%|██████▍   | 110M/170M [00:15<00:05, 10.7MB/s]\r 66%|██████▌   | 112M/170M [00:15<00:04, 13.0MB/s]\r 67%|██████▋   | 114M/170M [00:15<00:04, 12.2MB/s]\r 68%|██████▊   | 116M/170M [00:15<00:04, 11.8MB/s]\r 70%|██████▉   | 119M/170M [00:15<00:04, 12.7MB/s]\r 71%|███████   | 121M/170M [00:15<00:03, 16.0MB/s]\r 72%|███████▏  | 123M/170M [00:16<00:03, 13.8MB/s]\r 73%|███████▎  | 125M/170M [00:16<00:03, 13.0MB/s]\r 74%|███████▍  | 127M/170M [00:16<00:02, 14.6MB/s]\r 75%|███████▌  | 128M/170M [00:16<00:03, 13.9MB/s]\r 76%|███████▋  | 130M/170M [00:16<00:02, 13.8MB/s]\r 77%|███████▋  | 131M/170M [00:16<00:02, 13.7MB/s]\r 78%|███████▊  | 133M/170M [00:16<00:02, 14.9MB/s]\r 79%|███████▉  | 135M/170M [00:16<00:02, 12.9MB/s]\r 80%|████████  | 137M/170M [00:17<00:02, 12.1MB/s]\r 82%|████████▏ | 139M/170M [00:17<00:02, 15.3MB/s]\r 83%|████████▎ | 141M/170M [00:17<00:02, 14.0MB/s]\r 84%|████████▎ | 143M/170M [00:17<00:02, 13.9MB/s]\r 85%|████████▍ | 144M/170M [00:17<00:01, 13.8MB/s]\r 86%|████████▌ | 146M/170M [00:17<00:01, 12.6MB/s]\r 87%|████████▋ | 149M/170M [00:17<00:01, 15.2MB/s]\r 88%|████████▊ | 150M/170M [00:17<00:01, 15.5MB/s]\r 89%|████████▉ | 152M/170M [00:18<00:01, 15.5MB/s]\r 90%|████████▉ | 153M/170M [00:18<00:01, 15.5MB/s]\r 91%|█████████ | 155M/170M [00:18<00:01, 12.7MB/s]\r 93%|█████████▎| 158M/170M [00:18<00:00, 16.4MB/s]\r 94%|█████████▎| 160M/170M [00:18<00:00, 14.5MB/s]\r 95%|█████████▍| 161M/170M [00:18<00:00, 12.0MB/s]\r 96%|█████████▋| 164M/170M [00:19<00:00, 12.6MB/s]\r 98%|█████████▊| 167M/170M [00:19<00:00, 13.3MB/s]\r100%|█████████▉| 170M/170M [00:19<00:00, 13.4MB/s]\r100%|██████████| 170M/170M [00:19<00:00, 8.77MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化模型...\n模型总参数量: 3,250,122\n可训练参数量: 3,250,122\n训练样本数: 50,000\n验证样本数: 10,000\n开始训练 (50个epochs)...\n----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch [0/98], Loss: 2.3208\n  Batch [50/98], Loss: 1.7476\nEpoch [001/050] | Train Loss: 1.8286 | Val Loss: 1.4777 | Train Acc: 34.78% | Val Acc: 47.51% | LR: 0.200000 | ETA: 8.8min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 47.51% ***\n  Batch [0/98], Loss: 1.5597\n  Batch [50/98], Loss: 1.4537\nEpoch [002/050] | Train Loss: 1.4509 | Val Loss: 1.1329 | Train Acc: 47.89% | Val Acc: 59.02% | LR: 0.199803 | ETA: 8.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 59.02% ***\n  Batch [0/98], Loss: 1.3762\n  Batch [50/98], Loss: 1.1641\nEpoch [003/050] | Train Loss: 1.2135 | Val Loss: 0.9829 | Train Acc: 56.68% | Val Acc: 65.19% | LR: 0.199211 | ETA: 8.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 65.19% ***\n  Batch [0/98], Loss: 1.1200\n  Batch [50/98], Loss: 1.0747\nEpoch [004/050] | Train Loss: 1.0432 | Val Loss: 0.8253 | Train Acc: 62.92% | Val Acc: 71.29% | LR: 0.198229 | ETA: 8.4min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 71.29% ***\n  Batch [0/98], Loss: 1.0230\n  Batch [50/98], Loss: 0.8857\nEpoch [005/050] | Train Loss: 0.9220 | Val Loss: 0.7480 | Train Acc: 67.42% | Val Acc: 74.40% | LR: 0.196858 | ETA: 8.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 74.40% ***\n  Batch [0/98], Loss: 0.8267\n  Batch [50/98], Loss: 0.8808\nEpoch [006/050] | Train Loss: 0.8374 | Val Loss: 0.6727 | Train Acc: 70.55% | Val Acc: 76.55% | LR: 0.195106 | ETA: 8.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 76.55% ***\n  Batch [0/98], Loss: 0.7356\n  Batch [50/98], Loss: 0.7553\nEpoch [007/050] | Train Loss: 0.7758 | Val Loss: 0.6654 | Train Acc: 72.85% | Val Acc: 77.46% | LR: 0.192978 | ETA: 7.9min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 77.46% ***\n  Batch [0/98], Loss: 0.7934\n  Batch [50/98], Loss: 0.7572\nEpoch [008/050] | Train Loss: 0.7284 | Val Loss: 0.5676 | Train Acc: 74.66% | Val Acc: 80.63% | LR: 0.190483 | ETA: 7.8min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 80.63% ***\n  Batch [0/98], Loss: 0.7351\n  Batch [50/98], Loss: 0.6286\nEpoch [009/050] | Train Loss: 0.6722 | Val Loss: 0.5339 | Train Acc: 76.67% | Val Acc: 82.13% | LR: 0.187631 | ETA: 7.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 82.13% ***\n  Batch [0/98], Loss: 0.6473\n  Batch [50/98], Loss: 0.6899\nEpoch [010/050] | Train Loss: 0.6398 | Val Loss: 0.5085 | Train Acc: 77.76% | Val Acc: 83.14% | LR: 0.184433 | ETA: 7.4min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 83.14% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_10.pth\n  Batch [0/98], Loss: 0.5918\n  Batch [50/98], Loss: 0.5749\nEpoch [011/050] | Train Loss: 0.6052 | Val Loss: 0.4900 | Train Acc: 78.82% | Val Acc: 83.77% | LR: 0.180902 | ETA: 7.2min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 83.77% ***\n  Batch [0/98], Loss: 0.6322\n  Batch [50/98], Loss: 0.5839\nEpoch [012/050] | Train Loss: 0.5834 | Val Loss: 0.4683 | Train Acc: 79.84% | Val Acc: 84.28% | LR: 0.177051 | ETA: 7.0min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 84.28% ***\n  Batch [0/98], Loss: 0.4624\n  Batch [50/98], Loss: 0.5350\nEpoch [013/050] | Train Loss: 0.5544 | Val Loss: 0.4800 | Train Acc: 80.68% | Val Acc: 83.59% | LR: 0.172897 | ETA: 6.9min\n  Batch [0/98], Loss: 0.5142\n  Batch [50/98], Loss: 0.4638\nEpoch [014/050] | Train Loss: 0.5285 | Val Loss: 0.4544 | Train Acc: 81.66% | Val Acc: 84.87% | LR: 0.168455 | ETA: 6.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 84.87% ***\n  Batch [0/98], Loss: 0.5340\n  Batch [50/98], Loss: 0.5161\nEpoch [015/050] | Train Loss: 0.5185 | Val Loss: 0.4430 | Train Acc: 82.00% | Val Acc: 85.02% | LR: 0.163742 | ETA: 6.5min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 85.02% ***\n  Batch [0/98], Loss: 0.4777\n  Batch [50/98], Loss: 0.4638\nEpoch [016/050] | Train Loss: 0.4888 | Val Loss: 0.4332 | Train Acc: 82.98% | Val Acc: 85.19% | LR: 0.158779 | ETA: 6.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 85.19% ***\n  Batch [0/98], Loss: 0.5141\n  Batch [50/98], Loss: 0.4938\nEpoch [017/050] | Train Loss: 0.4765 | Val Loss: 0.4091 | Train Acc: 83.51% | Val Acc: 86.35% | LR: 0.153583 | ETA: 6.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 86.35% ***\n  Batch [0/98], Loss: 0.4663\n  Batch [50/98], Loss: 0.5049\nEpoch [018/050] | Train Loss: 0.4623 | Val Loss: 0.3989 | Train Acc: 84.09% | Val Acc: 86.55% | LR: 0.148175 | ETA: 5.9min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 86.55% ***\n  Batch [0/98], Loss: 0.4458\n  Batch [50/98], Loss: 0.4030\nEpoch [019/050] | Train Loss: 0.4468 | Val Loss: 0.4194 | Train Acc: 84.51% | Val Acc: 85.86% | LR: 0.142578 | ETA: 5.8min\n  Batch [0/98], Loss: 0.4473\n  Batch [50/98], Loss: 0.4565\nEpoch [020/050] | Train Loss: 0.4351 | Val Loss: 0.3901 | Train Acc: 84.99% | Val Acc: 86.73% | LR: 0.136812 | ETA: 5.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 86.73% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_20.pth\n  Batch [0/98], Loss: 0.3964\n  Batch [50/98], Loss: 0.4095\nEpoch [021/050] | Train Loss: 0.4178 | Val Loss: 0.3674 | Train Acc: 85.25% | Val Acc: 87.55% | LR: 0.130902 | ETA: 5.4min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 87.55% ***\n  Batch [0/98], Loss: 0.3688\n  Batch [50/98], Loss: 0.3846\nEpoch [022/050] | Train Loss: 0.4025 | Val Loss: 0.3731 | Train Acc: 85.92% | Val Acc: 87.60% | LR: 0.124869 | ETA: 5.2min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 87.60% ***\n  Batch [0/98], Loss: 0.3732\n  Batch [50/98], Loss: 0.3595\nEpoch [023/050] | Train Loss: 0.3963 | Val Loss: 0.3745 | Train Acc: 86.09% | Val Acc: 87.40% | LR: 0.118738 | ETA: 5.0min\n  Batch [0/98], Loss: 0.3660\n  Batch [50/98], Loss: 0.3825\nEpoch [024/050] | Train Loss: 0.3783 | Val Loss: 0.3637 | Train Acc: 86.93% | Val Acc: 87.96% | LR: 0.112533 | ETA: 4.8min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 87.96% ***\n  Batch [0/98], Loss: 0.3681\n  Batch [50/98], Loss: 0.3452\nEpoch [025/050] | Train Loss: 0.3666 | Val Loss: 0.3597 | Train Acc: 87.12% | Val Acc: 88.26% | LR: 0.106279 | ETA: 4.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.26% ***\n  Batch [0/98], Loss: 0.3044\n  Batch [50/98], Loss: 0.4045\nEpoch [026/050] | Train Loss: 0.3559 | Val Loss: 0.3338 | Train Acc: 87.71% | Val Acc: 88.77% | LR: 0.100000 | ETA: 4.5min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 88.77% ***\n  Batch [0/98], Loss: 0.3115\n  Batch [50/98], Loss: 0.3331\nEpoch [027/050] | Train Loss: 0.3403 | Val Loss: 0.3269 | Train Acc: 88.29% | Val Acc: 89.13% | LR: 0.093721 | ETA: 4.3min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.13% ***\n  Batch [0/98], Loss: 0.2868\n  Batch [50/98], Loss: 0.3191\nEpoch [028/050] | Train Loss: 0.3374 | Val Loss: 0.3416 | Train Acc: 88.22% | Val Acc: 88.65% | LR: 0.087467 | ETA: 4.1min\n  Batch [0/98], Loss: 0.3745\n  Batch [50/98], Loss: 0.3256\nEpoch [029/050] | Train Loss: 0.3214 | Val Loss: 0.3442 | Train Acc: 88.85% | Val Acc: 88.41% | LR: 0.081262 | ETA: 3.9min\n  Batch [0/98], Loss: 0.2581\n  Batch [50/98], Loss: 0.3230\nEpoch [030/050] | Train Loss: 0.3142 | Val Loss: 0.3067 | Train Acc: 88.99% | Val Acc: 89.68% | LR: 0.075131 | ETA: 3.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.68% ***\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_30.pth\n  Batch [0/98], Loss: 0.3120\n  Batch [50/98], Loss: 0.3085\nEpoch [031/050] | Train Loss: 0.3041 | Val Loss: 0.3077 | Train Acc: 89.38% | Val Acc: 89.95% | LR: 0.069098 | ETA: 3.5min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 89.95% ***\n  Batch [0/98], Loss: 0.2429\n  Batch [50/98], Loss: 0.2738\nEpoch [032/050] | Train Loss: 0.3006 | Val Loss: 0.3216 | Train Acc: 89.48% | Val Acc: 89.14% | LR: 0.063188 | ETA: 3.3min\n  Batch [0/98], Loss: 0.3332\n  Batch [50/98], Loss: 0.3252\nEpoch [033/050] | Train Loss: 0.2914 | Val Loss: 0.3138 | Train Acc: 89.76% | Val Acc: 89.93% | LR: 0.057422 | ETA: 3.2min\n  Batch [0/98], Loss: 0.2773\n  Batch [50/98], Loss: 0.2613\nEpoch [034/050] | Train Loss: 0.2810 | Val Loss: 0.3011 | Train Acc: 90.12% | Val Acc: 90.25% | LR: 0.051825 | ETA: 3.0min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.25% ***\n  Batch [0/98], Loss: 0.2692\n  Batch [50/98], Loss: 0.3036\nEpoch [035/050] | Train Loss: 0.2716 | Val Loss: 0.3043 | Train Acc: 90.52% | Val Acc: 90.07% | LR: 0.046417 | ETA: 2.8min\n  Batch [0/98], Loss: 0.2297\n  Batch [50/98], Loss: 0.3060\nEpoch [036/050] | Train Loss: 0.2617 | Val Loss: 0.2846 | Train Acc: 90.91% | Val Acc: 90.86% | LR: 0.041221 | ETA: 2.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 90.86% ***\n  Batch [0/98], Loss: 0.1839\n  Batch [50/98], Loss: 0.3116\nEpoch [037/050] | Train Loss: 0.2573 | Val Loss: 0.2905 | Train Acc: 91.07% | Val Acc: 90.54% | LR: 0.036258 | ETA: 2.5min\n  Batch [0/98], Loss: 0.2783\n  Batch [50/98], Loss: 0.2811\nEpoch [038/050] | Train Loss: 0.2478 | Val Loss: 0.2839 | Train Acc: 91.33% | Val Acc: 90.80% | LR: 0.031545 | ETA: 2.3min\n  Batch [0/98], Loss: 0.2530\n  Batch [50/98], Loss: 0.2351\nEpoch [039/050] | Train Loss: 0.2422 | Val Loss: 0.2814 | Train Acc: 91.58% | Val Acc: 90.69% | LR: 0.027103 | ETA: 2.1min\n  Batch [0/98], Loss: 0.2215\n  Batch [50/98], Loss: 0.2166\nEpoch [040/050] | Train Loss: 0.2393 | Val Loss: 0.2844 | Train Acc: 91.55% | Val Acc: 90.78% | LR: 0.022949 | ETA: 1.9min\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_40.pth\n  Batch [0/98], Loss: 0.2031\n  Batch [50/98], Loss: 0.2492\nEpoch [041/050] | Train Loss: 0.2277 | Val Loss: 0.2790 | Train Acc: 92.06% | Val Acc: 91.02% | LR: 0.019098 | ETA: 1.7min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.02% ***\n  Batch [0/98], Loss: 0.2258\n  Batch [50/98], Loss: 0.2621\nEpoch [042/050] | Train Loss: 0.2279 | Val Loss: 0.2746 | Train Acc: 91.92% | Val Acc: 91.23% | LR: 0.015567 | ETA: 1.5min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.23% ***\n  Batch [0/98], Loss: 0.1897\n  Batch [50/98], Loss: 0.2102\nEpoch [043/050] | Train Loss: 0.2220 | Val Loss: 0.2731 | Train Acc: 92.31% | Val Acc: 91.14% | LR: 0.012369 | ETA: 1.3min\n  Batch [0/98], Loss: 0.2223\n  Batch [50/98], Loss: 0.1869\nEpoch [044/050] | Train Loss: 0.2185 | Val Loss: 0.2758 | Train Acc: 92.36% | Val Acc: 91.25% | LR: 0.009517 | ETA: 1.1min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.25% ***\n  Batch [0/98], Loss: 0.2116\n  Batch [50/98], Loss: 0.2578\nEpoch [045/050] | Train Loss: 0.2135 | Val Loss: 0.2725 | Train Acc: 92.50% | Val Acc: 91.15% | LR: 0.007022 | ETA: 0.9min\n  Batch [0/98], Loss: 0.1773\n  Batch [50/98], Loss: 0.2096\nEpoch [046/050] | Train Loss: 0.2139 | Val Loss: 0.2725 | Train Acc: 92.64% | Val Acc: 91.20% | LR: 0.004894 | ETA: 0.8min\n  Batch [0/98], Loss: 0.2223\n  Batch [50/98], Loss: 0.1810\nEpoch [047/050] | Train Loss: 0.2112 | Val Loss: 0.2716 | Train Acc: 92.77% | Val Acc: 91.27% | LR: 0.003142 | ETA: 0.6min\n模型已保存到: /mnt/gpu_test/models/best_model.pth\n*** 新的最佳模型保存! 验证准确率: 91.27% ***\n  Batch [0/98], Loss: 0.2319\n  Batch [50/98], Loss: 0.2260\nEpoch [048/050] | Train Loss: 0.2120 | Val Loss: 0.2724 | Train Acc: 92.49% | Val Acc: 91.23% | LR: 0.001771 | ETA: 0.4min\n  Batch [0/98], Loss: 0.2328\n  Batch [50/98], Loss: 0.2084\nEpoch [049/050] | Train Loss: 0.2086 | Val Loss: 0.2722 | Train Acc: 92.55% | Val Acc: 91.26% | LR: 0.000789 | ETA: 0.2min\n  Batch [0/98], Loss: 0.2241\n  Batch [50/98], Loss: 0.2407\nEpoch [050/050] | Train Loss: 0.2087 | Val Loss: 0.2742 | Train Acc: 92.74% | Val Acc: 91.23% | LR: 0.000197 | ETA: 0.0min\n模型已保存到: /mnt/gpu_test/checkpoints/checkpoint_epoch_50.pth\n分布式训练资源已清理\n\n================================================================================\n=== 训练完成 ===\n最佳验证准确率: 91.27%\n峰值显存使用: 2.04 GB\n模型已保存到: /mnt/gpu_test/models/final_model.pth\n训练统计图已保存到: /mnt/gpu_test/logs/training_stats.png\n所有文件保存在: /mnt/gpu_test\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+torch_distributed_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        advanced_training_demo()\n",
    "    except Exception as e:\n",
    "        print(f\"训练过程中出现错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0193ff42-65e0-44c9-a23b-94b5d116ba6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPU_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}