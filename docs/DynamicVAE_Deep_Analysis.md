# DynamicVAE æ·±åº¦è§£æ

> æœ¬æ–‡æ¡£è¯¦ç»†è§£æ `DyAD/model/dynamic_vae.py` ä¸­çš„ `DynamicVAE` ç±»ï¼ŒåŒ…æ‹¬æ¨¡å—ç»“æ„ã€æ•°æ®æµã€æŸå¤±è®¡ç®—åŠç†è®ºè”ç³»ã€‚

---

## ç›®å½•

1. [ç±»ç»“æ„æ¦‚è§ˆ](#1-ç±»ç»“æ„æ¦‚è§ˆ)
2. [æ¨¡å—è¯¦ç»†è§£æ](#2-æ¨¡å—è¯¦ç»†è§£æ)
3. [å‰å‘ä¼ æ’­æ•°æ®æµ](#3-å‰å‘ä¼ æ’­æ•°æ®æµ)
4. [æŸå¤±å‡½æ•°è®¡ç®—](#4-æŸå¤±å‡½æ•°è®¡ç®—)
5. [ç†è®ºè”ç³»ï¼šDyAD ä¸åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡](#5-ç†è®ºè”ç³»dyad-ä¸åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡)
6. [å…³é”®ç­‰å¼æ±‡æ€»](#6-å…³é”®ç­‰å¼æ±‡æ€»)
7. [ä¼ªä»£ç ç‰ˆæœ¬](#7-ä¼ªä»£ç ç‰ˆæœ¬)
8. [æ•°å€¼ç¨³å®šæ€§é—®é¢˜ä¸æ”¹è¿›å»ºè®®](#8-æ•°å€¼ç¨³å®šæ€§é—®é¢˜ä¸æ”¹è¿›å»ºè®®)

---

## 1. ç±»ç»“æ„æ¦‚è§ˆ

### 1.1 ç±»å®šä¹‰

```python
class DynamicVAE(nn.Module):
    def __init__(self, rnn_type, hidden_size, latent_size, encoder_embedding_size, 
                 output_embedding_size, decoder_embedding_size, num_layers=1, 
                 bidirectional=False, variable_length=False, **params):
```

**ä½ç½®**: [dynamic_vae.py:L8-L11](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L8-L11)

### 1.2 æ ¸å¿ƒç»„ä»¶

| æ¨¡å—åç§° | ç±»å‹ | ä½œç”¨ | è¾“å…¥ç»´åº¦ | è¾“å‡ºç»´åº¦ |
|---------|------|------|---------|---------|
| `encoder_rnn` | RNN/LSTM/GRU | ç¼–ç æ—¶é—´åºåˆ— | `(batch, seq, encoder_emb)` | `(batch, seq, hidden)` + hidden state |
| `decoder_rnn` | RNN/LSTM/GRU | é‡æ„æ—¶é—´åºåˆ— | `(batch, seq, decoder_emb)` | `(batch, seq, hidden)` |
| `hidden2mean` | Linear | éšè—çŠ¶æ€â†’æ½œåœ¨å‡å€¼ | `(batch, hidden*factor)` | `(batch, latent_size)` |
| `hidden2log_v` | Linear | éšè—çŠ¶æ€â†’æ½œåœ¨å¯¹æ•°æ–¹å·® | `(batch, hidden*factor)` | `(batch, latent_size)` |
| `latent2hidden` | Linear | æ½œåœ¨å‘é‡â†’è§£ç å™¨éšè—çŠ¶æ€ | `(batch, latent_size)` | `(batch, hidden*factor)` |
| `outputs2embedding` | Linear | è§£ç å™¨è¾“å‡ºâ†’é‡æ„åµŒå…¥ | `(batch, seq, hidden*dir)` | `(batch, seq, output_emb)` |
| `mean2latent` | Sequential | æ½œåœ¨å‡å€¼â†’æ ‡ç­¾é¢„æµ‹ | `(batch, latent_size)` | `(batch, 1)` |

**æ³¨**ï¼š
- `factor = (2 if bidirectional else 1) * num_layers`ï¼Œåœ¨ [L25](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L25) å®šä¹‰
- `dir = 2 if bidirectional else 1`ï¼Œç”¨äº `outputs2embedding`

---

## 2. æ¨¡å—è¯¦ç»†è§£æ

### 2.1 Encoder RNN

**ä»£ç ä½ç½®**: [L20-L21](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L20-L21)

```python
self.encoder_rnn = rnn(encoder_embedding_size, hidden_size, num_layers=num_layers,
                       bidirectional=self.bidirectional, batch_first=True)
```

**åŠŸèƒ½**ï¼š
- å°†è¾“å…¥æ—¶é—´åºåˆ—æ˜ å°„åˆ°éšè—çŠ¶æ€ç©ºé—´
- æ”¯æŒå˜é•¿åºåˆ—ï¼ˆé€šè¿‡ `pack_padded_sequence`ï¼‰

**ç»´åº¦å˜åŒ–**ï¼š
```
è¾“å…¥: (batch_size, seq_len, encoder_embedding_size)
  â†“ encoder_rnn
è¾“å‡º output: (batch_size, seq_len, hidden_size * (2 if bidirectional else 1))
è¾“å‡º hidden: (num_layers * (2 if bidirectional else 1), batch_size, hidden_size)
```

**å¼ é‡é‡å¡‘** ([L41-L44](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L41-L44)):
```python
if self.bidirectional or self.num_layers > 1:
    hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)
else:
    hidden = hidden.squeeze()
```
æœ€ç»ˆ `hidden` ç»´åº¦ï¼š`(batch_size, hidden_size * hidden_factor)`

---

### 2.2 Variational Bottleneckï¼ˆå˜åˆ†ç“¶é¢ˆï¼‰

#### 2.2.1 Mean & Log Variance Projection

**ä»£ç ä½ç½®**: [L27-L28](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L27-L28)

```python
self.hidden2mean = nn.Linear(hidden_size * self.hidden_factor, latent_size)
self.hidden2log_v = nn.Linear(hidden_size * self.hidden_factor, latent_size)
```

**ç»´åº¦å˜åŒ–**ï¼š
```
hidden: (batch, hidden*factor) 
  â†“ hidden2mean
mean: (batch, latent_size)

hidden: (batch, hidden*factor)
  â†“ hidden2log_v
log_v: (batch, latent_size)
```

**æ‰§è¡Œä½ç½®** ([L46-L48](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L46-L48)):
```python
mean = self.hidden2mean(hidden)
log_v = self.hidden2log_v(hidden)
std = torch.exp(0.5 * log_v)
```

#### 2.2.2 Reparameterization Trick

**ä»£ç ä½ç½®**: [L51-L55](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L51-L55)

```python
z = to_var(torch.randn([batch_size, self.latent_size]))
if self.training:
    z = z * std * noise_scale + mean  # è®­ç»ƒæ—¶ï¼šÎ¼ + ÏƒÂ·Îµ
else:
    z = mean                          # æµ‹è¯•æ—¶ï¼šç›´æ¥ä½¿ç”¨å‡å€¼
```

**æ•°å­¦è¡¨è¾¾å¼**ï¼š
$$
z = \begin{cases}
\mu + \sigma \cdot \epsilon \cdot \text{noise\_scale}, & \text{è®­ç»ƒæ—¶} \\
\mu, & \text{æµ‹è¯•æ—¶}
\end{cases}
\quad \text{å…¶ä¸­ } \epsilon \sim \mathcal{N}(0, I)
$$

**ç»´åº¦**ï¼š
```
z: (batch, latent_size)
```

---

### 2.3 Latent to Decoder Hidden

**ä»£ç ä½ç½®**: [L29](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L29), [L56-L61](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L56-L61)

```python
self.latent2hidden = nn.Linear(latent_size, hidden_size * self.hidden_factor)

# åœ¨ forward() ä¸­ï¼š
hidden = self.latent2hidden(z)
if self.bidirectional or self.num_layers > 1:
    hidden = hidden.view(self.hidden_factor, batch_size, self.hidden_size)
else:
    hidden = hidden.unsqueeze(0)
```

**ç»´åº¦å˜åŒ–**ï¼š
```
z: (batch, latent_size)
  â†“ latent2hidden
hidden: (batch, hidden*factor)
  â†“ reshape
hidden: (num_layers*directions, batch, hidden_size)
```

---

### 2.4 Decoder RNN

**ä»£ç ä½ç½®**: [L22-L23](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L22-L23), [L63-L71](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L63-L71)

```python
self.decoder_rnn = rnn(decoder_embedding_size, hidden_size, num_layers=num_layers,
                       bidirectional=self.bidirectional, batch_first=True)

# åœ¨ forward() ä¸­ï¼š
de_input_sequence = decoder_filter(input_sequence)  # æå–è§£ç å™¨è¾“å…¥ç‰¹å¾
de_input_embedding = de_input_sequence.to(torch.float32)
if self.variable_length:
    de_input_embedding = pack_padded_sequence(de_input_embedding, seq_lengths, batch_first=True)
    outputs, _ = self.decoder_rnn(de_input_embedding, hidden)
    outputs, _ = pad_packed_sequence(outputs, batch_first=True)
else:
    outputs, _ = self.decoder_rnn(de_input_embedding, hidden)
```

**ç»´åº¦å˜åŒ–**ï¼š
```
de_input_embedding: (batch, seq, decoder_embedding_size)
  â†“ decoder_rnn (åˆå§‹éšè—çŠ¶æ€ç”± latent2hidden æä¾›)
outputs: (batch, seq, hidden_size * (2 if bidirectional else 1))
```

---

### 2.5 Output to Embedding

**ä»£ç ä½ç½®**: [L30](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L30), [L72](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L72)

```python
self.outputs2embedding = nn.Linear(hidden_size * (2 if bidirectional else 1), output_embedding_size)

# åœ¨ forward() ä¸­ï¼š
log_p = self.outputs2embedding(outputs)
```

**ç»´åº¦å˜åŒ–**ï¼š
```
outputs: (batch, seq, hidden*directions)
  â†“ outputs2embedding
log_p: (batch, seq, output_embedding_size)
```

**ä½œç”¨**ï¼šå°†è§£ç å™¨çš„éšè—çŠ¶æ€æŠ•å½±åˆ°è¾“å‡ºç©ºé—´ï¼ˆå¦‚åŸå§‹ç‰¹å¾ç»´åº¦ï¼‰ï¼Œç”¨äºé‡æ„

---

### 2.6 Mean to Latent (Label Prediction)

**ä»£ç ä½ç½®**: [L31-L32](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L31-L32), [L49](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L49)

```python
self.mean2latent = nn.Sequential(
    nn.Linear(latent_size, int(hidden_size / 2)), 
    nn.ReLU(),
    nn.Linear(int(hidden_size / 2), 1)
)

# åœ¨ forward() ä¸­ï¼š
mean_pred = self.mean2latent(mean)
```

**ç»´åº¦å˜åŒ–**ï¼š
```
mean: (batch, latent_size)
  â†“ Linear(latent_size â†’ hidden/2) â†’ ReLU â†’ Linear(hidden/2 â†’ 1)
mean_pred: (batch, 1)
```

**ä½œç”¨**ï¼šä»æ½œåœ¨å‡å€¼é¢„æµ‹æ ‡ç­¾ï¼ˆå¦‚é‡Œç¨‹ mileageï¼‰ï¼Œå®ç°ç›‘ç£å­¦ä¹ çš„è¾…åŠ©ç›®æ ‡

---

## 3. å‰å‘ä¼ æ’­æ•°æ®æµ

### 3.1 å®Œæ•´æµç¨‹å›¾

```mermaid
graph TD
    A[input_sequence<br/>batchÃ—seqÃ—features] --> B[encoder_filter]
    B --> C[encoder_rnn<br/>batchÃ—seqÃ—encoder_emb]
    C --> D[hidden state<br/>batchÃ—hidden*factor]
    D --> E1[hidden2mean<br/>batchÃ—latent]
    D --> E2[hidden2log_v<br/>batchÃ—latent]
    E1 --> F[mean Î¼]
    E2 --> G[log_v<br/>â†’ std Ïƒ]
    F --> H[Reparameterization<br/>z = Î¼ + ÏƒÂ·Îµ]
    G --> H
    H --> I[latent2hidden<br/>batchÃ—hidden*factor]
    I --> J[reshape<br/>layersÃ—batchÃ—hidden]
    J --> K[decoder_rnn input]
    A --> L[decoder_filter]
    L --> K
    K --> M[decoder_rnn<br/>batchÃ—seqÃ—hidden*dir]
    M --> N[outputs2embedding<br/>batchÃ—seqÃ—output_emb]
    N --> O[log_p é‡æ„è¾“å‡º]
    F --> P[mean2latent<br/>batchÃ—1]
    P --> Q[mean_pred æ ‡ç­¾é¢„æµ‹]
    
    O --> R[Loss: NLL + KL + Label]
    Q --> R
```

### 3.2 é€è¡Œä»£ç æµç¨‹

**ä»£ç ä½ç½®**: [forward() æ–¹æ³• L34-L73](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L34-L73)

```python
def forward(self, input_sequence, encoder_filter, decoder_filter, seq_lengths, noise_scale=1.0):
    # Step 1: è·å–æ‰¹æ¬¡å¤§å°
    batch_size = input_sequence.size(0)  # L35
    
    # Step 2: Encoder è·¯å¾„
    en_input_sequence = encoder_filter(input_sequence)  # L36 - æå–ç¼–ç å™¨ç‰¹å¾
    en_input_embedding = en_input_sequence.to(torch.float32)  # L37
    if self.variable_length:
        en_input_embedding = pack_padded_sequence(en_input_embedding, seq_lengths, batch_first=True)  # L39
    output, hidden = self.encoder_rnn(en_input_embedding)  # L40 - RNN ç¼–ç 
    
    # Step 3: é‡å¡‘éšè—çŠ¶æ€
    if self.bidirectional or self.num_layers > 1:
        hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)  # L42
    else:
        hidden = hidden.squeeze()  # L44
    
    # Step 4: å˜åˆ†æ¨æ–­
    mean = self.hidden2mean(hidden)  # L46 - Î¼
    log_v = self.hidden2log_v(hidden)  # L47 - log(ÏƒÂ²)
    std = torch.exp(0.5 * log_v)  # L48 - Ïƒ = exp(0.5 * log(ÏƒÂ²))
    mean_pred = self.mean2latent(mean)  # L49 - æ ‡ç­¾é¢„æµ‹
    
    # Step 5: é‡å‚æ•°åŒ–é‡‡æ ·
    z = to_var(torch.randn([batch_size, self.latent_size]))  # L51
    if self.training:
        z = z * std * noise_scale + mean  # L53
    else:
        z = mean  # L55
    
    # Step 6: æ½œåœ¨å‘é‡åˆ°è§£ç å™¨éšè—çŠ¶æ€
    hidden = self.latent2hidden(z)  # L56
    if self.bidirectional or self.num_layers > 1:
        hidden = hidden.view(self.hidden_factor, batch_size, self.hidden_size)  # L59
    else:
        hidden = hidden.unsqueeze(0)  # L61
    
    # Step 7: Decoder è·¯å¾„
    de_input_sequence = decoder_filter(input_sequence)  # L63 - æå–è§£ç å™¨ç‰¹å¾
    de_input_embedding = de_input_sequence.to(torch.float32)  # L64
    if self.variable_length:
        de_input_embedding = pack_padded_sequence(de_input_embedding, seq_lengths, batch_first=True)  # L66
        outputs, _ = self.decoder_rnn(de_input_embedding, hidden)  # L68
        outputs, _ = pad_packed_sequence(outputs, batch_first=True)  # L69
    else:
        outputs, _ = self.decoder_rnn(de_input_embedding, hidden)  # L71
    
    # Step 8: è¾“å‡ºæŠ•å½±
    log_p = self.outputs2embedding(outputs)  # L72 - é‡æ„è¾“å‡º
    
    return log_p, mean, log_v, z, mean_pred  # L73
```

### 3.3 å…³é”®ç»´åº¦å˜åŒ–æ€»ç»“

| é˜¶æ®µ | å˜é‡å | ç»´åº¦ | ä»£ç è¡Œ |
|-----|--------|------|--------|
| è¾“å…¥ | `input_sequence` | `(B, T, F)` | L34 |
| ç¼–ç å™¨è¾“å…¥ | `en_input_embedding` | `(B, T, E_enc)` | L37 |
| ç¼–ç å™¨éšè— | `hidden` (é‡å¡‘å) | `(B, H*factor)` | L42/L44 |
| æ½œåœ¨å‡å€¼ | `mean` | `(B, L)` | L46 |
| æ½œåœ¨å¯¹æ•°æ–¹å·® | `log_v` | `(B, L)` | L47 |
| æ ‡å‡†å·® | `std` | `(B, L)` | L48 |
| æ½œåœ¨å‘é‡ | `z` | `(B, L)` | L51-55 |
| è§£ç å™¨åˆå§‹éšè— | `hidden` (é‡å¡‘å) | `(factor, B, H)` | L59/L61 |
| è§£ç å™¨è¾“å…¥ | `de_input_embedding` | `(B, T, E_dec)` | L64 |
| è§£ç å™¨è¾“å‡º | `outputs` | `(B, T, H*dir)` | L68/L71 |
| é‡æ„è¾“å‡º | `log_p` | `(B, T, O)` | L72 |
| æ ‡ç­¾é¢„æµ‹ | `mean_pred` | `(B, 1)` | L49 |

**ç¬¦å·è¯´æ˜**ï¼š
- `B` = batch_size
- `T` = sequence length
- `F` = æ€»ç‰¹å¾æ•°
- `E_enc` = encoder_embedding_size
- `E_dec` = decoder_embedding_size
- `H` = hidden_size
- `L` = latent_size
- `O` = output_embedding_size
- `factor` = `num_layers * (2 if bidirectional else 1)`
- `dir` = `2 if bidirectional else 1`

---

## 4. æŸå¤±å‡½æ•°è®¡ç®—

### 4.1 æ€»æŸå¤±æ„æˆ

**ä»£ç ä½ç½®**: [train.py:L136-L140](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L136-L140)

```python
nll_loss, kl_loss, kl_weight = self.loss_fn(log_p, target, mean, log_v)  # L136
self.label_data = tasks.Label(column_name="mileage", training_set=train)  # L137
label_loss = self.label_data.loss(batch, mean_pred, is_mse=True)  # L138
loss = (self.args.nll_weight * nll_loss + 
        self.args.latent_label_weight * label_loss + 
        kl_weight * kl_loss / batch_.shape[0])  # L139-L140
```

**æ€»æŸå¤±å…¬å¼**ï¼š
$$
\mathcal{L}_{\text{total}} = w_{\text{nll}} \cdot \mathcal{L}_{\text{NLL}} + w_{\text{label}} \cdot \mathcal{L}_{\text{label}} + w_{\text{kl}}(t) \cdot \frac{\mathcal{L}_{\text{KL}}}{B}
$$

å…¶ä¸­ï¼š
- $w_{\text{nll}}$ = `args.nll_weight` (é‡æ„æƒé‡)
- $w_{\text{label}}$ = `args.latent_label_weight` (æ ‡ç­¾ç›‘ç£æƒé‡)
- $w_{\text{kl}}(t)$ = `kl_weight` (KL é€€ç«æƒé‡ï¼Œéšè®­ç»ƒæ­¥æ•°å˜åŒ–)
- $B$ = `batch_size`

---

### 4.2 é‡æ„æŸå¤± (NLL Loss)

**ä»£ç ä½ç½®**: [train.py:L203-L216](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L203-L216)

```python
def loss_fn(self, log_p, target, mean, log_v):
    nll = torch.nn.SmoothL1Loss(reduction='mean')  # L212
    nll_loss = nll(log_p, target)  # L213
    # ...
    return nll_loss, kl_loss, kl_weight
```

**å…¬å¼**ï¼š
$$
\mathcal{L}_{\text{NLL}} = \frac{1}{B \cdot T \cdot O} \sum_{i=1}^{B} \sum_{t=1}^{T} \sum_{o=1}^{O} \text{SmoothL1}(\hat{x}_{i,t,o}, x_{i,t,o})
$$

å…¶ä¸­ SmoothL1 (Huber Loss)ï¼š
$$
\text{SmoothL1}(a, b) = \begin{cases}
0.5 \cdot (a - b)^2, & \text{if } |a - b| < 1 \\
|a - b| - 0.5, & \text{otherwise}
\end{cases}
$$

**è¾“å…¥**ï¼š
- `log_p`: æ¨¡å‹é‡æ„è¾“å‡º `(B, T, O)`
- `target`: ç›®æ ‡åºåˆ— `(B, T, O)`ï¼Œç”± `data_task.target_filter(batch_)` æå– ([train.py:L134](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L134))

**ä½œç”¨**ï¼šè¡¡é‡é‡æ„è´¨é‡ï¼Œé©±åŠ¨æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„ç¼–ç -è§£ç 

---

### 4.3 KL æ•£åº¦æŸå¤±

**ä»£ç ä½ç½®**: [train.py:L214-L216](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L214-L216)

```python
def loss_fn(self, log_p, target, mean, log_v):
    # ...
    kl_loss = -0.5 * torch.sum(1 + log_v - mean.pow(2) - log_v.exp())  # L214
    kl_weight = self.kl_anneal_function()  # L215
    return nll_loss, kl_loss, kl_weight
```

**å…¬å¼**ï¼š
$$
\mathcal{L}_{\text{KL}} = -\frac{1}{2} \sum_{i=1}^{B} \sum_{j=1}^{L} \left( 1 + \log(\sigma_{ij}^2) - \mu_{ij}^2 - \sigma_{ij}^2 \right)
$$

è¿™æ˜¯ KL æ•£åº¦ $D_{\text{KL}}(q(z|x) \| p(z))$ çš„è§£æå½¢å¼ï¼Œå…¶ä¸­ï¼š
- $q(z|x) = \mathcal{N}(\mu, \text{diag}(\sigma^2))$ (ç¼–ç å™¨åˆ†å¸ƒ)
- $p(z) = \mathcal{N}(0, I)$ (æ ‡å‡†æ­£æ€å…ˆéªŒ)

**æ¨å¯¼**ï¼š
$$
D_{\text{KL}}(q \| p) = \int q(z|x) \log \frac{q(z|x)}{p(z)} dz
$$
å¯¹äºé«˜æ–¯åˆ†å¸ƒï¼Œè§£æè§£ä¸ºï¼š
$$
= \frac{1}{2} \sum_{j=1}^{L} \left( \mu_j^2 + \sigma_j^2 - \log(\sigma_j^2) - 1 \right)
$$

ä»£ç ä¸­ä½¿ç”¨è´Ÿå·å¹¶å‡å»å¸¸æ•°é¡¹ï¼Œä¿æŒæ•°å­¦ä¸€è‡´æ€§ã€‚

**KL é€€ç«æƒé‡** ([train.py:L218-L227](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L218-L227)):
```python
def kl_anneal_function(self):
    if self.args.anneal_function == 'logistic':
        return self.args.anneal0 * float(1 / (1 + np.exp(-self.args.k * (self.step - self.args.x0))))  # L223
    elif self.args.anneal_function == 'linear':
        return self.args.anneal0 * min(1, self.step / self.args.x0)  # L225
    else:
        return self.args.anneal0  # L227
```

**é€€ç«ç­–ç•¥**ï¼š
- **Logistic**: $w_{\text{kl}}(t) = w_0 \cdot \frac{1}{1 + e^{-k(t - x_0)}}$
- **Linear**: $w_{\text{kl}}(t) = w_0 \cdot \min(1, \frac{t}{x_0})$
- **Constant**: $w_{\text{kl}}(t) = w_0$

**ä½œç”¨**ï¼šç¼“è§£"åéªŒåå¡Œ"é—®é¢˜ï¼ŒåˆæœŸé™ä½ KL æƒé‡ï¼Œè®©æ¨¡å‹å…ˆå­¦ä¹ é‡æ„ï¼ŒåæœŸé€æ¸å¢å¼ºæ­£åˆ™åŒ–

---

### 4.4 æ ‡ç­¾æŸå¤± (Label Loss)

**ä»£ç ä½ç½®**: [tasks.py:L15-L27](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/tasks.py#L15-L27)

```python
def loss(self, batch, mean_pred, is_mse=True):
    label_data = []
    for i in batch[1][self.label]:  # L17
        norm_label = (i - self.min_mileage) / (self.max_mileage - self.min_mileage)  # L18
        label_data.append(norm_label)
    label = torch.tensor(label_data)  # L20
    x = mean_pred.squeeze().to("cuda")  # L21 - é¢„æµ‹å€¼
    y = label.float().to("cuda")  # L22 - çœŸå®å€¼
    mse = torch.nn.MSELoss(reduction='mean')  # L23
    loss = 0
    if is_mse:
        loss = mse(x, y)  # L26
    return loss
```

**å…¬å¼**ï¼š
$$
\mathcal{L}_{\text{label}} = \frac{1}{B} \sum_{i=1}^{B} \left( \text{mean\_pred}_i - y_i^{\text{norm}} \right)^2
$$

å…¶ä¸­æ ‡ç­¾å½’ä¸€åŒ–ï¼š
$$
y_i^{\text{norm}} = \frac{y_i - y_{\min}}{y_{\max} - y_{\min}}
$$

**è¾“å…¥**ï¼š
- `mean_pred`: ä»æ½œåœ¨å‡å€¼é¢„æµ‹çš„æ ‡ç­¾ `(B, 1)` ([dynamic_vae.py:L49](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L49))
- `batch[1][self.label]`: çœŸå®æ ‡ç­¾ï¼ˆå¦‚ mileageï¼‰

**ä½œç”¨**ï¼šç›‘ç£å­¦ä¹ è¾…åŠ©ä»»åŠ¡ï¼Œç¡®ä¿æ½œåœ¨ç©ºé—´æ•è·ä¸æ ‡ç­¾ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚é‡Œç¨‹ã€å¥åº·çŠ¶æ€ï¼‰

---

### 4.5 æŸå¤±è®¡ç®—æ€»æµç¨‹

```mermaid
graph LR
    A[Model Forward] --> B[log_p, mean, log_v, mean_pred]
    B --> C[NLL Loss<br/>SmoothL1log_p, target]
    B --> D[KL Loss<br/>-0.5 * Î£1+log_v-meanÂ²-exp log_v]
    B --> E[Label Loss<br/>MSEmean_pred, label_norm]
    D --> F[KL Anneal<br/>kl_weight]
    C --> G[Weighted Sum]
    E --> G
    F --> G
    G --> H[Total Loss<br/>w_nllÂ·NLL + w_labelÂ·Label + kl_weightÂ·KL/B]
    H --> I[Backward + Optimizer Step]
```

---

## 5. ç†è®ºè”ç³»ï¼šDyAD ä¸åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡

### 5.1 DyAD æ ¸å¿ƒæ€æƒ³

æ ¹æ® Nature Communications è®ºæ–‡ä¸­çš„æè¿°ï¼Œ**DyAD (Dynamic Autoencoder for Anomaly Detection)** æ—¨åœ¨ï¼š
1. **åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡**ï¼šä½¿ç”¨ RNN/LSTM æ•è·æ—¶é—´åºåˆ—çš„æ—¶åºä¾èµ–å’Œæ¼”åŒ–è§„å¾‹
2. **æ½œåœ¨ç©ºé—´è§£è€¦**ï¼šé€šè¿‡ VAE å°†è§‚æµ‹æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œåˆ†ç¦»æ­£å¸¸ä¸å¼‚å¸¸æ¨¡å¼
3. **ç¤¾ä¼š/ç»æµå› ç´ é…ç½®**ï¼šå…è®¸å¼•å…¥å¤–éƒ¨ç›‘ç£ä¿¡å·ï¼ˆå¦‚ä½¿ç”¨åœºæ™¯ã€ç¯å¢ƒå› ç´ ã€é‡Œç¨‹ç­‰ï¼‰ï¼Œå¢å¼ºæ½œåœ¨ç©ºé—´çš„å¯è§£é‡Šæ€§

### 5.2 ä»£ç å®ç°çš„å¯¹åº”å…³ç³»

#### 5.2.1 åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡

**å®ç°æ–¹å¼**ï¼š
- **Encoder RNN** ([L20-L21](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L20-L21))ï¼šæ•è·æ—¶é—´åºåˆ—çš„åŠ¨æ€æ¼”åŒ–
- **Decoder RNN** ([L22-L23](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L22-L23))ï¼šä»æ½œåœ¨çŠ¶æ€é‡æ„æ—¶é—´è·¯å¾„
- **å˜é•¿åºåˆ—æ”¯æŒ** ([L38-L39](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L38-L39), [L65-L69](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L65-L69))ï¼šé€‚åº”ä¸åŒé•¿åº¦çš„ç”µæ± å……æ”¾ç”µå‘¨æœŸ

**ç†è®ºè”ç³»**ï¼š
$$
\text{State Evolution: } h_t = f(h_{t-1}, x_t; \theta_{\text{enc}})
$$
RNN éšè—çŠ¶æ€ $h_t$ å»ºæ¨¡äº†ç³»ç»Ÿåœ¨æ—¶åˆ» $t$ çš„åŠ¨æ€çŠ¶æ€

#### 5.2.2 æ½œåœ¨ç©ºé—´çš„æ¦‚ç‡å»ºæ¨¡

**å®ç°æ–¹å¼**ï¼š
- **å˜åˆ†æ¨æ–­** ([L46-L48](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L46-L48))ï¼šä¼°è®¡æ½œåœ¨åˆ†å¸ƒ $q_\phi(z|x) = \mathcal{N}(\mu, \sigma^2)$
- **é‡å‚æ•°åŒ–** ([L51-L55](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L51-L55))ï¼šå…è®¸åå‘ä¼ æ’­
- **å…ˆéªŒæ­£åˆ™åŒ–** ([train.py:L214](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L214))ï¼šKL æ•£åº¦çº¦æŸæ½œåœ¨ç©ºé—´ç»“æ„

**ç†è®ºè”ç³»**ï¼šVAE æ¡†æ¶ä½¿æ½œåœ¨ç©ºé—´å…·æœ‰è¿ç»­æ€§å’Œå¯æ’å€¼æ€§ï¼Œä¾¿äºå¼‚å¸¸æ£€æµ‹ï¼š
$$
\text{Anomaly Score: } \mathcal{L}_{\text{rec}}(x) + \beta \cdot D_{\text{KL}}(q_\phi(z|x) \| p(z))
$$

#### 5.2.3 ç¤¾ä¼š/ç»æµå› ç´ é…ç½®

**å®ç°æ–¹å¼**ï¼š
- **`mean2latent` æ¨¡å—** ([L31-L32](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L31-L32), [L49](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L49))ï¼šä»æ½œåœ¨å‡å€¼é¢„æµ‹æ ‡ç­¾
- **Label Loss** ([train.py:L138](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L138), [tasks.py:L15-L27](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/tasks.py#L15-L27))ï¼šç›‘ç£å­¦ä¹ é‡Œç¨‹ (mileage)
- **å¯æ‰©å±•æ¥å£**ï¼š`tasks.Label` æ”¯æŒé€šè¿‡ `column_name` æŒ‡å®šä¸åŒæ ‡ç­¾ ([tasks.py:L8-L9](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/tasks.py#L8-L9))

**ä»£ç ä¸­çš„å‚æ•°/æ¥å£**ï¼š
1. **ä»»åŠ¡é…ç½®** ([train.py:L90-L91](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L90-L91)):
   ```python
   self.args.columns = torch.load(os.path.join(os.path.dirname(self.args.train_path), "column.pkl"))
   self.data_task = tasks.Task(task_name=self.args.task, columns=self.args.columns)
   ```
   é€šè¿‡ `task_name` (å¦‚ 'ev', 'batterybrandb') é€‰æ‹©ä¸åŒç‰¹å¾é…ç½®

2. **æ ‡ç­¾åˆ—æŒ‡å®š** ([train.py:L137](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L137)):
   ```python
   self.label_data = tasks.Label(column_name="mileage", training_set=train)
   ```
   å¯æ›¿æ¢ä¸ºå…¶ä»–åˆ—ï¼ˆå¦‚æ¸©åº¦ã€SOCã€ä½¿ç”¨åœºæ™¯ç­‰ï¼‰

3. **æƒé‡æ§åˆ¶** ([train.py:L139](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L139)):
   ```python
   loss = (self.args.nll_weight * nll_loss + 
           self.args.latent_label_weight * label_loss + ...)
   ```
   é€šè¿‡ `latent_label_weight` è°ƒæ•´ç›‘ç£å¼ºåº¦

**ç†è®ºè”ç³»**ï¼š
è¿™ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ç¡®ä¿æ½œåœ¨ç©ºé—´ä¸ä»…æ•è·æ•°æ®çš„å†…åœ¨ç»“æ„ï¼Œè¿˜å¯¹å¤–éƒ¨å› ç´ æ•æ„Ÿï¼Œç¬¦åˆè®ºæ–‡ä¸­"é…ç½®ç¤¾ä¼š/ç»æµå› ç´ "çš„æ€æƒ³ã€‚

### 5.3 DyAD åœ¨ç”µæ± æ•…éšœæ£€æµ‹ä¸­çš„åº”ç”¨

**æ•°æ®æµ**ï¼š
1. ç”µæ± æ—¶é—´åºåˆ—ï¼ˆSOC, ç”µæµ, æ¸©åº¦, ç”µå‹ç­‰ï¼‰â†’ Encoder â†’ æ½œåœ¨è¡¨ç¤º
2. æ½œåœ¨è¡¨ç¤º â†’ Decoder â†’ é‡æ„åºåˆ—
3. é‡æ„è¯¯å·®é«˜ â†’ å¼‚å¸¸ï¼ˆæ•…éšœå‰å…†ï¼‰

**ç›‘ç£ä¿¡å·**ï¼š
- **é‡Œç¨‹ (mileage)**ï¼šåæ˜ ç”µæ± è€åŒ–ç¨‹åº¦ï¼ŒæŒ‡å¯¼æ½œåœ¨ç©ºé—´å­¦ä¹ å¥åº·çŠ¶æ€è½´
- **ï¼ˆæ½œåœ¨æ‰©å±•ï¼‰ä½¿ç”¨åœºæ™¯/ç¯å¢ƒå› ç´ **ï¼šå¯é€šè¿‡ä¿®æ”¹ `Label` å¼•å…¥ï¼ˆå¦‚å¿«å……é¢‘ç‡ã€æ¸©åº¦åŒºé—´ç­‰ï¼‰

---

## 6. å…³é”®ç­‰å¼æ±‡æ€»

### 6.1 å‰å‘ä¼ æ’­

| æ­¥éª¤ | ç­‰å¼ | ä»£ç è¡Œ |
|-----|------|--------|
| ç¼–ç å™¨ | $h = \text{RNN}_{\text{enc}}(x_{\text{enc}})$ | [L40](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L40) |
| æ½œåœ¨å‡å€¼ | $\mu = W_\mu h + b_\mu$ | [L46](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L46) |
| æ½œåœ¨å¯¹æ•°æ–¹å·® | $\log \sigma^2 = W_{\log \sigma} h + b_{\log \sigma}$ | [L47](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L47) |
| æ ‡å‡†å·® | $\sigma = \exp(0.5 \cdot \log \sigma^2)$ | [L48](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L48) |
| é‡å‚æ•°åŒ– | $z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$ | [L53](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L53) |
| è§£ç å™¨åˆå§‹çŠ¶æ€ | $h_0^{\text{dec}} = W_z z + b_z$ | [L56](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L56) |
| è§£ç å™¨ | $o = \text{RNN}_{\text{dec}}(x_{\text{dec}}, h_0^{\text{dec}})$ | [L68/L71](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L68-L71) |
| é‡æ„è¾“å‡º | $\hat{x} = W_o o + b_o$ | [L72](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L72) |
| æ ‡ç­¾é¢„æµ‹ | $\hat{y} = \text{MLP}(\mu)$ | [L49](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L49) |

### 6.2 æŸå¤±å‡½æ•°

$$
\begin{aligned}
\mathcal{L}_{\text{NLL}} &= \frac{1}{B \cdot T \cdot O} \sum \text{SmoothL1}(\hat{x}, x) \\[10pt]
\mathcal{L}_{\text{KL}} &= -\frac{1}{2} \sum_{i,j} \left( 1 + \log \sigma_{ij}^2 - \mu_{ij}^2 - \sigma_{ij}^2 \right) \\[10pt]
\mathcal{L}_{\text{label}} &= \frac{1}{B} \sum_{i} \left( \hat{y}_i - y_i^{\text{norm}} \right)^2 \\[10pt]
\mathcal{L}_{\text{total}} &= w_{\text{nll}} \cdot \mathcal{L}_{\text{NLL}} + w_{\text{label}} \cdot \mathcal{L}_{\text{label}} + w_{\text{kl}}(t) \cdot \frac{\mathcal{L}_{\text{KL}}}{B}
\end{aligned}
$$

### 6.3 KL é€€ç«

**Logistic**:
$$
w_{\text{kl}}(t) = w_0 \cdot \frac{1}{1 + \exp(-k(t - x_0))}
$$

**Linear**:
$$
w_{\text{kl}}(t) = w_0 \cdot \min\left(1, \frac{t}{x_0}\right)
$$

---

## 7. ä¼ªä»£ç ç‰ˆæœ¬

```pseudocode
# ============================================
#  DynamicVAE: å¸¦ç›‘ç£å­¦ä¹ çš„åŠ¨æ€å˜åˆ†è‡ªç¼–ç å™¨
# ============================================

INPUT: 
  - input_sequence: æ—¶é—´åºåˆ— (batch, seq_len, features)
  - encoder_filter: ç¼–ç å™¨ç‰¹å¾é€‰æ‹©å‡½æ•°
  - decoder_filter: è§£ç å™¨ç‰¹å¾é€‰æ‹©å‡½æ•°
  - seq_lengths: åºåˆ—é•¿åº¦åˆ—è¡¨ (ç”¨äºå˜é•¿åºåˆ—)
  - noise_scale: å™ªå£°ç¼©æ”¾å› å­ (é»˜è®¤ 1.0)

OUTPUT:
  - log_p: é‡æ„åºåˆ— (batch, seq_len, output_dim)
  - mean: æ½œåœ¨å‡å€¼ (batch, latent_size)
  - log_v: æ½œåœ¨å¯¹æ•°æ–¹å·® (batch, latent_size)
  - z: é‡‡æ ·çš„æ½œåœ¨å‘é‡ (batch, latent_size)
  - mean_pred: æ ‡ç­¾é¢„æµ‹ (batch, 1)

# -------- ENCODER é˜¶æ®µ --------
1. æå–ç¼–ç å™¨è¾“å…¥ç‰¹å¾:
   en_input = encoder_filter(input_sequence)  # (batch, seq, encoder_emb)

2. é€šè¿‡ Encoder RNN:
   IF variable_length:
       en_input = pack_padded_sequence(en_input, seq_lengths)
   output, hidden = encoder_rnn(en_input)
   
3. é‡å¡‘éšè—çŠ¶æ€ä¸º 2D:
   IF bidirectional OR num_layers > 1:
       hidden = reshape(hidden, [batch, hidden_size * hidden_factor])
   ELSE:
       hidden = squeeze(hidden)

# -------- å˜åˆ†æ¨æ–­é˜¶æ®µ --------
4. è®¡ç®—æ½œåœ¨åˆ†å¸ƒå‚æ•°:
   mean = Linear_mean(hidden)           # (batch, latent_size)
   log_v = Linear_log_v(hidden)         # (batch, latent_size)
   std = exp(0.5 * log_v)

5. æ ‡ç­¾é¢„æµ‹:
   mean_pred = MLP(mean)                # (batch, 1)

6. é‡å‚æ•°åŒ–é‡‡æ ·:
   epsilon ~ N(0, I)
   IF training:
       z = mean + std * epsilon * noise_scale
   ELSE:
       z = mean

# -------- DECODER é˜¶æ®µ --------
7. æ½œåœ¨å‘é‡åˆ°è§£ç å™¨åˆå§‹éšè—çŠ¶æ€:
   hidden_dec = Linear_latent(z)       # (batch, hidden*factor)
   IF bidirectional OR num_layers > 1:
       hidden_dec = reshape(hidden_dec, [hidden_factor, batch, hidden_size])
   ELSE:
       hidden_dec = unsqueeze(hidden_dec, dim=0)

8. æå–è§£ç å™¨è¾“å…¥ç‰¹å¾:
   de_input = decoder_filter(input_sequence)  # (batch, seq, decoder_emb)

9. é€šè¿‡ Decoder RNN:
   IF variable_length:
       de_input = pack_padded_sequence(de_input, seq_lengths)
       outputs, _ = decoder_rnn(de_input, hidden_dec)
       outputs = pad_packed_sequence(outputs)
   ELSE:
       outputs, _ = decoder_rnn(de_input, hidden_dec)

10. è¾“å‡ºæŠ•å½±:
    log_p = Linear_output(outputs)     # (batch, seq, output_dim)

RETURN log_p, mean, log_v, z, mean_pred


# ============================================
#  æŸå¤±è®¡ç®—
# ============================================

GIVEN:
  - log_p: é‡æ„è¾“å‡º (batch, seq, output_dim)
  - target: ç›®æ ‡åºåˆ— (batch, seq, output_dim)
  - mean, log_v: æ½œåœ¨åˆ†å¸ƒå‚æ•° (batch, latent_size)
  - mean_pred: æ ‡ç­¾é¢„æµ‹ (batch, 1)
  - label: çœŸå®æ ‡ç­¾ (batch,)

1. é‡æ„æŸå¤±:
   nll_loss = SmoothL1Loss(log_p, target)

2. KL æ•£åº¦:
   kl_loss = -0.5 * SUM(1 + log_v - mean^2 - exp(log_v))
   kl_weight = anneal_function(step)  # é€€ç«æƒé‡

3. æ ‡ç­¾æŸå¤±:
   label_norm = (label - label_min) / (label_max - label_min)
   label_loss = MSE(mean_pred.squeeze(), label_norm)

4. æ€»æŸå¤±:
   total_loss = w_nll * nll_loss + w_label * label_loss + kl_weight * kl_loss / batch_size

UPDATE parameters via backpropagation
```

---

## 8. æ•°å€¼ç¨³å®šæ€§é—®é¢˜ä¸æ”¹è¿›å»ºè®®

### 8.1 æ½œåœ¨é—®é¢˜

#### é—®é¢˜ 1: å¯¹æ•°æ–¹å·®çš„æ•°å€¼ä¸ç¨³å®š

**ä½ç½®**: [L48](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L48)

```python
std = torch.exp(0.5 * log_v)
```

**é£é™©**ï¼š
- å¦‚æœ `log_v` è¿‡å¤§ï¼ˆå¦‚ > 10ï¼‰ï¼Œ`exp(0.5 * log_v)` ä¼šå¯¼è‡´æ•°å€¼æº¢å‡º
- å¦‚æœ `log_v` è¿‡å°ï¼ˆå¦‚ < -10ï¼‰ï¼Œæ ‡å‡†å·®æ¥è¿‘ 0ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ–¹æ³• 1: è£å‰ªå¯¹æ•°æ–¹å·®
log_v = torch.clamp(log_v, min=-10, max=10)
std = torch.exp(0.5 * log_v)

# æ–¹æ³• 2: ç›´æ¥é¢„æµ‹æ ‡å‡†å·®çš„å¯¹æ•°
self.hidden2log_std = nn.Linear(...)  # é¢„æµ‹ log(Ïƒ) è€Œé log(ÏƒÂ²)
log_std = self.hidden2log_std(hidden)
log_std = torch.clamp(log_std, min=-5, max=5)
std = torch.exp(log_std)
```

---

#### é—®é¢˜ 2: KL æ•£åº¦è®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§

**ä½ç½®**: [train.py:L214](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/train.py#L214)

```python
kl_loss = -0.5 * torch.sum(1 + log_v - mean.pow(2) - log_v.exp())
```

**é£é™©**ï¼š
- `log_v.exp()` å¯èƒ½æº¢å‡º
- `mean.pow(2)` å¯¹äºå¤§å‡å€¼ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸

**æ”¹è¿›å»ºè®®**ï¼š
```python
# é€å…ƒç´ è£å‰ªï¼Œé¿å…æç«¯å€¼
mean = torch.clamp(mean, min=-10, max=10)
log_v = torch.clamp(log_v, min=-10, max=10)
kl_loss = -0.5 * torch.sum(1 + log_v - mean.pow(2) - log_v.exp())

# æˆ–ä½¿ç”¨ PyTorch å†…ç½® KL æ•£åº¦
from torch.distributions import Normal, kl_divergence
q_z = Normal(mean, torch.exp(0.5 * log_v))
p_z = Normal(torch.zeros_like(mean), torch.ones_like(mean))
kl_loss = kl_divergence(q_z, p_z).sum()
```

---

#### é—®é¢˜ 3: é‡å‚æ•°åŒ–æ—¶çš„å™ªå£°å°ºåº¦

**ä½ç½®**: [L51-L53](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L51-L53)

```python
z = to_var(torch.randn([batch_size, self.latent_size]))
if self.training:
    z = z * std * noise_scale + mean
```

**é£é™©**ï¼š
- å¦‚æœ `noise_scale` è®¾ç½®ä¸å½“ï¼Œå¯èƒ½ç ´åè®­ç»ƒç¨³å®šæ€§
- æµ‹è¯•æ—¶ç›´æ¥ä½¿ç”¨ `mean` å¯èƒ½å¯¼è‡´åˆ†å¸ƒåç§»

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ–¹æ³• 1: æ·»åŠ æœ€å°å™ªå£°ï¼Œå³ä½¿åœ¨æµ‹è¯•æ—¶
if self.training:
    z = mean + std * torch.randn_like(std) * noise_scale
else:
    z = mean + std * torch.randn_like(std) * 0.1  # å°å™ªå£°ä¿æŒç”Ÿæˆå¤šæ ·æ€§

# æ–¹æ³• 2: ä½¿ç”¨ PyTorch åˆ†å¸ƒ
from torch.distributions import Normal
q_z = Normal(mean, std * noise_scale if self.training else std * 0.1)
z = q_z.rsample()  # å¯å¾®é‡‡æ ·
```

---

#### é—®é¢˜ 4: å˜é•¿åºåˆ—çš„å¡«å……å¤„ç†

**ä½ç½®**: [L38-L39](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L38-L39), [L65-L69](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/dynamic_vae.py#L65-L69)

**é£é™©**ï¼š
- å¡«å……ä½ç½®çš„é‡æ„è¯¯å·®ä¼šè´¡çŒ®åˆ°æŸå¤±ä¸­ï¼Œæ±¡æŸ“æ¢¯åº¦

**æ”¹è¿›å»ºè®®**ï¼š
```python
# åœ¨è®¡ç®—æŸå¤±æ—¶ä½¿ç”¨ mask
def loss_fn(self, log_p, target, mean, log_v, seq_lengths=None):
    if seq_lengths is not None:
        # åˆ›å»º mask
        max_len = log_p.size(1)
        mask = torch.arange(max_len).expand(len(seq_lengths), max_len) < seq_lengths.unsqueeze(1)
        mask = mask.unsqueeze(-1).to(log_p.device)
        
        # ä»…è®¡ç®—æœ‰æ•ˆä½ç½®çš„æŸå¤±
        diff = (log_p - target) * mask
        nll_loss = F.smooth_l1_loss(diff, torch.zeros_like(diff), reduction='sum')
        nll_loss /= mask.sum()  # å½’ä¸€åŒ–
    else:
        nll_loss = F.smooth_l1_loss(log_p, target, reduction='mean')
    
    kl_loss = -0.5 * torch.sum(1 + log_v - mean.pow(2) - log_v.exp())
    kl_weight = self.kl_anneal_function()
    return nll_loss, kl_loss, kl_weight
```

---

#### é—®é¢˜ 5: æ ‡ç­¾å½’ä¸€åŒ–çš„ç¨³å®šæ€§

**ä½ç½®**: [tasks.py:L18](file:///Users/David/Desktop/github_repos/Battery_fault_detection_NC_github/DyAD/model/tasks.py#L18)

```python
norm_label = (i - self.min_mileage) / (self.max_mileage - self.min_mileage)
```

**é£é™©**ï¼š
- å¦‚æœ `max_mileage == min_mileage`ï¼Œä¼šå¯¼è‡´é™¤ä»¥é›¶
- å½’ä¸€åŒ–èŒƒå›´å›ºå®šä¸º [0, 1]ï¼Œå¯èƒ½ä¸é€‚åˆæ‰€æœ‰åœºæ™¯

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ–¹æ³• 1: æ·»åŠ æ•°å€¼ç¨³å®šé¡¹
norm_label = (i - self.min_mileage) / (self.max_mileage - self.min_mileage + 1e-8)

# æ–¹æ³• 2: ä½¿ç”¨æ ‡å‡†åŒ–ï¼ˆå‡å€¼ 0ï¼Œæ–¹å·® 1ï¼‰
self.mean_mileage = np.mean(self.sample_mileage)
self.std_mileage = np.std(self.sample_mileage) + 1e-8
norm_label = (i - self.mean_mileage) / self.std_mileage

# æ–¹æ³• 3: ä½¿ç”¨é²æ£’å½’ä¸€åŒ–ï¼ˆä¸­ä½æ•° + IQRï¼‰
self.median_mileage = np.median(self.sample_mileage)
self.iqr_mileage = np.percentile(self.sample_mileage, 75) - np.percentile(self.sample_mileage, 25) + 1e-8
norm_label = (i - self.median_mileage) / self.iqr_mileage
```

---

### 8.2 è®­ç»ƒç¨³å®šæ€§æ”¹è¿›

#### å»ºè®® 1: æ¢¯åº¦è£å‰ª

```python
# åœ¨ train.py çš„ä¼˜åŒ–å™¨æ­¥éª¤å‰æ·»åŠ 
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

#### å»ºè®® 2: è‡ªé€‚åº” KL é€€ç«

```python
def adaptive_kl_anneal(self, kl_value, target_kl=0.5):
    """æ ¹æ® KL æ•£åº¦å€¼è‡ªé€‚åº”è°ƒæ•´æƒé‡"""
    if kl_value < target_kl * 0.8:
        self.kl_weight *= 1.05  # å¢åŠ æƒé‡
    elif kl_value > target_kl * 1.2:
        self.kl_weight *= 0.95  # å‡å°‘æƒé‡
    return self.kl_weight
```

#### å»ºè®® 3: ç›‘æ§å…³é”®æŒ‡æ ‡

åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼š
```python
# ç›‘æ§ KL æ•£åº¦ã€é‡æ„è¯¯å·®ã€æ½œåœ¨ç©ºé—´ç»Ÿè®¡
with torch.no_grad():
    mean_std = torch.mean(std).item()
    mean_kl = kl_loss.item() / batch_size
    if mean_std < 0.01:  # åéªŒåå¡Œ
        print(f"Warning: Posterior collapse detected (std={mean_std:.4f})")
    if mean_kl > 50:  # KL çˆ†ç‚¸
        print(f"Warning: KL divergence too large (KL={mean_kl:.4f})")
```

#### å»ºè®® 4: é¢„çƒ­ç­–ç•¥

```python
# å‰ N ä¸ª epoch åªè®­ç»ƒé‡æ„ï¼Œä¸ä½¿ç”¨ KL
if self.current_epoch <= self.args.warmup_epochs:
    kl_weight = 0.0
else:
    kl_weight = self.kl_anneal_function()
```

---

### 8.3 ä»£ç é‡æ„å»ºè®®

#### å»ºè®® 1: åˆ†ç¦»è®¡ç®—ä¸æ§åˆ¶æµ

å°† `forward()` ä¸­çš„æ¡ä»¶åˆ¤æ–­å°è£…ï¼š
```python
def _reshape_hidden_for_latent(self, hidden, batch_size):
    if self.bidirectional or self.num_layers > 1:
        return hidden.view(batch_size, self.hidden_size * self.hidden_factor)
    else:
        return hidden.squeeze()

def _reshape_hidden_for_decoder(self, hidden, batch_size):
    if self.bidirectional or self.num_layers > 1:
        return hidden.view(self.hidden_factor, batch_size, self.hidden_size)
    else:
        return hidden.unsqueeze(0)
```

#### å»ºè®® 2: æ·»åŠ ç±»å‹æ³¨è§£

```python
from typing import Tuple, Optional
import torch
from torch import Tensor

def forward(
    self, 
    input_sequence: Tensor,  # (B, T, F)
    encoder_filter: Callable[[Tensor], Tensor],
    decoder_filter: Callable[[Tensor], Tensor],
    seq_lengths: Optional[Tensor] = None,  # (B,)
    noise_scale: float = 1.0
) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    """
    Returns:
        log_p: (B, T, O) - é‡æ„è¾“å‡º
        mean: (B, L) - æ½œåœ¨å‡å€¼
        log_v: (B, L) - æ½œåœ¨å¯¹æ•°æ–¹å·®
        z: (B, L) - é‡‡æ ·çš„æ½œåœ¨å‘é‡
        mean_pred: (B, 1) - æ ‡ç­¾é¢„æµ‹
    """
    ...
```

#### å»ºè®® 3: å•å…ƒæµ‹è¯•

```python
def test_dimension_consistency():
    model = DynamicVAE(
        rnn_type='lstm',
        hidden_size=128,
        latent_size=32,
        encoder_embedding_size=6,
        decoder_embedding_size=2,
        output_embedding_size=4
    )
    
    batch_size, seq_len = 16, 100
    x = torch.randn(batch_size, seq_len, 6)
    
    # Mock filters
    enc_filter = lambda x: x[:, :, :6]
    dec_filter = lambda x: x[:, :, :2]
    
    log_p, mean, log_v, z, mean_pred = model(x, enc_filter, dec_filter)
    
    assert log_p.shape == (batch_size, seq_len, 4)
    assert mean.shape == (batch_size, 32)
    assert log_v.shape == (batch_size, 32)
    assert z.shape == (batch_size, 32)
    assert mean_pred.shape == (batch_size, 1)
    print("âœ“ All dimension checks passed")
```

---

## æ€»ç»“

### æ ¸å¿ƒè®¾è®¡äº®ç‚¹

1. **åŠ¨æ€å»ºæ¨¡èƒ½åŠ›**ï¼šé€šè¿‡ RNN æ•è·æ—¶é—´ä¾èµ–ï¼Œé€‚åˆç”µæ± æ—¶é—´åºåˆ—
2. **æ¦‚ç‡è§£è€¦**ï¼šVAE å°†è§‚æµ‹åˆ†ç¦»ä¸ºç¡®å®šæ€§ï¼ˆmeanï¼‰å’Œéšæœºæ€§ï¼ˆstdï¼‰
3. **ç›‘ç£å¢å¼º**ï¼š`mean2latent` å¼•å…¥æ ‡ç­¾ï¼Œå¢å¼ºæ½œåœ¨ç©ºé—´çš„è¯­ä¹‰æ€§
4. **çµæ´»æ¶æ„**ï¼šæ”¯æŒåŒå‘ã€å¤šå±‚ã€å˜é•¿åºåˆ—

### æ”¹è¿›ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | æ”¹è¿›é¡¹ | å½±å“ |
|--------|--------|------|
| ğŸ”´ é«˜ | å¯¹æ•°æ–¹å·®è£å‰ª ([8.1 é—®é¢˜1](#é—®é¢˜-1-å¯¹æ•°æ–¹å·®çš„æ•°å€¼ä¸ç¨³å®š)) | é˜²æ­¢æ•°å€¼æº¢å‡º |
| ğŸ”´ é«˜ | å˜é•¿åºåˆ— mask ([8.1 é—®é¢˜4](#é—®é¢˜-4-å˜é•¿åºåˆ—çš„å¡«å……å¤„ç†)) | é¿å…å¡«å……æ±¡æŸ“æ¢¯åº¦ |
| ğŸŸ¡ ä¸­ | æ¢¯åº¦è£å‰ª ([8.2 å»ºè®®1](#å»ºè®®-1-æ¢¯åº¦è£å‰ª)) | æå‡è®­ç»ƒç¨³å®šæ€§ |
| ğŸŸ¡ ä¸­ | æ ‡ç­¾å½’ä¸€åŒ–ç¨³å®šæ€§ ([8.1 é—®é¢˜5](#é—®é¢˜-5-æ ‡ç­¾å½’ä¸€åŒ–çš„ç¨³å®šæ€§)) | é¿å…é™¤é›¶é”™è¯¯ |
| ğŸŸ¢ ä½ | ç±»å‹æ³¨è§£ ([8.3 å»ºè®®2](#å»ºè®®-2-æ·»åŠ ç±»å‹æ³¨è§£)) | æå‡ä»£ç å¯è¯»æ€§ |

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2025-11-24  
**å¯¹åº”ä»£ç ç‰ˆæœ¬**: æœ€æ–°ä¸»åˆ†æ”¯  
**ä½œè€…**: AI Code Analyst
