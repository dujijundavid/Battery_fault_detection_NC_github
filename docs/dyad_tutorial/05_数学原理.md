# 第4章：数学原理 - VAE 完整推导

> **章节定位**: 变分自编码器 (VAE) 的完整数学推导和理论基础
>
> **预计学习时间**: 2-3 小时
>
> **难度等级**: ★★★★★ (最抽象)

---

## 目录

- [4.1 ELBO 推导](#41-elbo-推导)
- [4.2 损失函数推导](#42-损失函数推导)
- [4.3 重参数化推导](#43-重参数化推导)
- [4.4 KL 散度计算](#44-kl-散度计算)

---

## 4.1 ELBO 推导

### 问题设定

我们想最大化观测数据的对数似然：

$$
\max_\theta \log p_\theta(x) = \sum_x \log p_\theta(x)
$$

但边际似然难以计算（需要对潜变量积分）：

$$
\log p_\theta(x) = \log \int p_\theta(x|z)p(z)dz
$$

### 引入近似分布

引入变分分布 $q_\phi(z|x)$ 来近似真实后验 $p_\theta(z|x)$：

$$
q_\phi(z|x) \approx p_\theta(z|x)
$$

### 推导 ELBO

使用 Jensen 不等式：

$$
\begin{aligned}
\log p_\theta(x) &= \log \int q_\phi(z|x) \frac{p_\theta(x,z)}{q_\phi(z|x)} dz \\
&= \log \mathbb{E}_{z \sim q} \left[\frac{p_\theta(x,z)}{q_\phi(z|x)}\right] \\
&\geq \mathbb{E}_{z \sim q}[\log p_\theta(x,z)] \\
&= \mathbb{E}_{z \sim q}[\log p_\theta(x|z) + \log q_\phi(z|x) - \log q_\phi(z|x)]
\end{aligned}
$$

得到**证据下界 (ELBO)**：

$$
\mathcal{L}(\phi, \theta) = \underbrace{\mathbb{E}_{z \sim q}[\log p_\theta(x|z)]}_{\text{重构项}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{KL 项}}
$$

---

## 4.2 损失函数推导

### 重构损失

假设高斯似然：

$$
\log p_\theta(x|z) = -\frac{1}{2\sigma^2}\|x - \mu\|^2 + \text{const}
$$

取期望：

$$
\mathbb{E}[\log p_\theta(x|z)] = -\mathbb{E}\left[\frac{1}{2\sigma^2}\|x - \mu\|^2\right]
$$

### KL 散度项

$$
D_{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2}\sum_{j=1}^d \left(1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2\right)
$$

### 总损失

$$
\mathcal{L}_{total} = \mathcal{L}_{recon} + \beta \cdot \mathcal{L}_{KL}
$$

其中 $\beta$ 是 KL 权重（退火参数）。

---

## 4.3 重参数化推导

### 目标

使采样 $z = \mu + \sigma \odot \varepsilon$ 可微分。

### 梯度计算

$$
\begin{aligned}
z &= \mu + \sigma \odot \varepsilon \\
\frac{\partial z}{\partial \mu} &= \sigma \odot \varepsilon \odot \mathbf{1} = \sigma \odot \varepsilon \\
\frac{\partial z}{\partial \sigma} &= \mu \odot \varepsilon \odot \mathbf{1} = \mu \odot \varepsilon
\end{aligned}
$$

关键：梯度可以通过 $\mu$ 和 $\sigma$ 回传！

---

## 4.4 KL 散度计算

### 一维高斯 KL 散度

$$
D_{KL}(\mathcal{N}(\mu_q, \sigma_q^2) \| \mathcal{N}(0, 1)) = \log \sigma_q - \mu_q^2 - \sigma_q^2 + \frac{1}{2}
$$

### DyAD 中的计算

```python
# dynamic_vae.py 第 214 行
kl_loss = -0.5 * torch.sum(1 + log_v - mean.pow(2) - log_v.exp())

# 其中:
# mean: [batch, latent_dim]  均值向量
# log_v: [batch, latent_dim]  对数方差
# 公式对应: -0.5 * Σ(1 + log(σ²) - μ² - σ²)
```

---

## 本章小结

学习完本章后，您应该能够：

- [ ] 推导 VAE 的 ELBO 目标函数
- [ ] 理解重构损失和 KL 散度的数学形式
- [ ] 证明重参数化技巧的梯度回传
- [ ] 计算高斯分布的 KL 散度
- [ ] 理解 DyAD 损失函数的数学基础

### 自测题

#### Q1: 为什么需要引入变分分布 q 而不是直接优化 p(z|x)？

<details>
<summary>点击查看答案</summary>

**答案**: 因为真实的后验 p(z|x) 在神经网络中难以计算（需要积分），而引入变分分布 q 将问题转化为优化问题。

</details>

---

**回到教程首页**: [README.md](./README.md)

**章节版本**: v1.0
**最后更新**: 2025-02-12
