# 附录A：DyAD 调参最佳实践

> **附录定位**: 基于实践经验的 DyAD 模型调参指南，涵盖超参数、训练技巧、常见问题
>
> **预计学习时间**: 2-3 小时
>
> **难度等级**: ★★★☆☆ (需要一定训练经验)
>
> **来源**: 基于多个项目实践总结

---

## 目录

- [A.1 超参数详解](#a1-超参数详解)
- [A.2 参数搜索策略](#a2-参数搜索策略)
- [A.3 常见陷阱与解决方案](#a3-常见陷阱与解决方案)
- [A.4 不同数据规模的推荐配置](#a4-不同数据规模的推荐配置)

---

## A.1 超参数详解

### A.1.1 模型架构参数

```
┌─────────────────────────────────────────────────────────────┐
│              DyAD 模型架构超参数                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. hidden_size (隐藏层维度)                                   │
│     ├── 默认: 128                                              │
│     ├── 可选范围: [32, 64, 128, 256]                        │
│     ├── 影响:                                                  │
│     │   ├─ 越大: 表达能力越强，但训练越慢                       │
│     │   └─ 越小: 训练快，但可能欠拟合                            │
│     └── 推荐策略: 根据数据复杂度选择                               │
│         简单数据 → 64, 复杂数据 → 128-256                     │
│                                                             │
│  2. latent_size (潜在维度)                                      │
│     ├── 默认: 8                                                │
│     ├── 可选范围: [4, 8, 16, 32]                               │
│     ├── 影响:                                                  │
│     │   ├─ 太小: 信息压缩过度，重构差                               │
│     │   ├─ 太大: KL 难以约束，生成质量差                         │
│     │   └─ 适中: 平衡压缩能力和表达力                              │
│     └── 推荐策略: 从 8 开始，重构误差大则增加到 16                   │
│                                                             │
│  3. num_layers (RNN层数)                                       │
│     ├── 默认: 1 (单向)                                          │
│     ├── 可选范围: [1, 2, 3]                                     │
│     ├── 影响:                                                  │
│     │   ├─ 越多: 越深的时序依赖，但梯度消失越严重               │
│     │   └─ 推荐: 1-2 层，单向解码器不需要太深                     │
│     └── 推荐策略: 编码器双向×1层 + 解码器单向×1层                  │
│                                                             │
│  4. bidirectional (双向编码)                                     │
│     ├── 默认: True (编码器), False (解码器)                       │
│     ├── 影响:                                                  │
│     │   ├─ True: 能看到完整上下文，参数×2                           │
│     │   └─ False: 参数少，推理快                                   │
│     └── 推荐策略: 编码器用双向（捕获上下文），解码器用单向（条件生成）│
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### A.1.2 训练参数

```
┌─────────────────────────────────────────────────────────────┐
│              DyAD 训练超参数                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. learning_rate (学习率)                                      │
│     ├── 默认: 0.001                                             │
│     ├── 可选范围: [0.0001, 0.0005, 0.001, 0.005, 0.01]          │
│     ├── 影响:                                                  │
│     │   ├─ 太大: 训练不稳定，震荡                                  │
│     │   ├─ 太小: 收敛极慢                                         │
│     │   └─ 适中: 稳定快速收敛                                   │
│     └── 推荐策略:                                            │
│         ├── 从 0.001 开始                                          │
│         ├── 使用学习率调度: ReduceLROnPlateau (factor=0.5)             │
│         └── 监控 loss 曲线，震荡则降低                                  │
│                                                             │
│  2. batch_size (批大小)                                        │
│     ├── 默认: 32 或 64                                          │
│     ├── 可选范围: [16, 32, 64, 128, 256]                       │
│     ├── 影响:                                                  │
│     │   ├─ 越大: 训练稳定，梯度估计准，但内存需求大                 │
│     │   ├─ 越小: 内存友好，但训练不稳定                            │
│     │   └─ 权衡: GPU 内存限制 vs 训练稳定性                      │
│     └── 推荐策略:                                            │
│         ├── GPU 内存 8GB: batch_size = 64-128                         │
│         ├── GPU 内存 16GB: batch_size = 128-256                        │
│         └── 确保一个 batch 包含多个循环                               │
│                                                             │
│  3. epochs (训练轮数)                                          │
│     ├── 默认: 100                                               │
│     ├── 可选范围: [50, 100, 200, 500]                            │
│     ├── 影响:                                                  │
│     │   ├─ 太少: 欠拟合                                         │
│     │   ├─ 太多: 过拟合，浪费时间                                   │
│     │   └─ 适中: Early Stopping 自动确定                           │
│     └── 推荐策略:                                            │
│         ├── 使用 Early Stopping (patience=10)                         │
│         ├── 监控验证集 loss                                       │
│         └── 保存最佳模型                                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### A.1.3 损失函数权重

```
┌─────────────────────────────────────────────────────────────┐
│           DyAD 损失函数权重                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  L_total = w1×L_NLL + w2×L_Label + β×L_KL                      │
│                                                             │
│  1. nll_weight (重构损失权重)                                  │
│     ├── 默认: 1.0                                               │
│     ├── 作用: 重构损失的主导程度                                  │
│     ├── 调参策略:                                            │
│     │   ├── 通常固定为 1.0 (作为基准)                              │
│     │   ├── 如重构不理想 → 可尝试增大到 1.5                          │
│     │   └── 如 KL 过小 → 可尝试减小到 0.5                           │
│     └── 注意: 与 KL 权重联动调整                                     │
│                                                             │
│  2. latent_label_weight (辅助预测权重)                             │
│     ├── 默认: 0.01                                              │
│     ├── 作用: 标签预测任务的重要性                                  │
│     ├── 调参策略:                                            │
│     │   ├── 小值 (0.001-0.01): 辅助作用弱，主任务主导               │
│     │   ├── 中值 (0.01-0.1): 辅助作用适中                          │
│     │   └── 大值 (>0.1): 强制学习标签，可能影响重构                   │
│     └── 推荐: 从 0.01 开始，观察标签预测 loss                         │
│                                                             │
│  3. KL 权重 (通过退火函数控制)                                   │
│     ├── 退火方式: logistic 或 linear                                  │
│     ├── logistic 参数:                                            │
│     │   ├── anneal0: 最大权重 (通常 1.0)                            │
│     │   ├── k: 增长率 (0.0005-0.005)                            │
│     │   └── x0: 中点位置 (总步数的 20-50%)                       │
│     └── linear 参数:                                            │
│         └── anneal0: 达到最大权重的步数 (总步数的 50-80%)              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## A.2 参数搜索策略

### A.2.1 网格搜索 vs 随机搜索

```
┌─────────────────────────────────────────────────────────────┐
│            参数搜索方法对比                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  网格搜索 (Grid Search):                                       │
│  ├── 方法: 遍历所有参数组合                                     │
│  ├── 优点: 保证找到最优                                         │
│  ├── 缺点: 计算量大, 指数增长                                  │
│  └── 适用: 超参数少 (<5个) 时                                   │
│                                                             │
│  随机搜索 (Random Search):                                     │
│  ├── 方法: 随机采样参数空间                                     │
│  ├── 优点: 效率高, 适合多参数                                    │
│  ├── 缺点: 不保证最优                                         │
│  └── 适用: 超参数多 (>5个) 时                                   │
│                                                             │
│  Bayesian 优化 (推荐):                                          │
│  ├── 方法: 根据历史结果选择下一组参数                            │
│  ├── 优点: 效率高, 智能探索                                  │
│  ├── 工具: Optuna, Hyperopt, Ray Tune                           │
│  └── 适用: 计算资源有限时                                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### A.2.2 推荐搜索流程

```python
# DyAD 超参数优化流程

import optuna

def objective(trial):
    """Optuna 目标函数"""

    # 1. 建议超参数
    params = {
        'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256]),
        'latent_size': trial.suggest_categorical('latent_size', [4, 8, 16]),
        'lr': trial.suggest_loguniform('lr', 1e-4, 1e-2),
        'nll_weight': trial.suggest_uniform('nll_weight', 0.5, 2.0),
        'latent_label_weight': trial.suggest_loguniform('latent_label_weight', 1e-3, 1e-1),
        # KL 退火参数
        'kl_anneal0': trial.suggest_categorical('kl_anneal0', [5000, 10000, 20000]),
        'kl_k': trial.suggest_float('kl_k', 0.0005, 0.005),
    }

    # 2. 训练模型
    model = DyAD(**params)
    val_loss = train_and_validate(model, params)

    return val_loss  # 最小化验证损失

# 3. 运行优化
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

# 4. 获取最佳参数
best_params = study.best_trial.params
print(f"最佳参数: {best_params}")

# 5. 用最佳参数重新训练
final_model = train_with_best_params(best_params)
```

---

### A.2.3 分阶段调参策略

```
┌─────────────────────────────────────────────────────────────┐
│            分阶段调参策略 (推荐)                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  第1阶段: 确定模型架构                                        │
│  ├── 固定: 学习率、batch_size、优化器                            │
│  ├── 搜索: hidden_size, latent_size, num_layers                   │
│  ├── 评估: 快速训练 10-20 epochs，观察 loss 下降曲线              │
│  └── 目标: 找到合适的模型大小                                   │
│                                                             │
│  第2阶段: 优化训练参数                                          │
│  ├── 固定: 第1阶段确定的模型架构                                 │
│  ├── 搜索: learning_rate, batch_size                              │
│  ├── 评估: 完整训练到收敛，观察验证集指标                       │
│  └── 目标: 找到稳定高效的训练配置                                 │
│                                                             │
│  第3阶段: 精调损失权重                                          │
│  ├── 固定: 前两阶段确定的所有参数                               │
│  ├── 搜索: nll_weight, latent_label_weight, KL 退火参数                │
│  ├── 评估: 关注验证集重构误差和 KL 值的平衡                       │
│  └── 目标: 找到最优的损失权重配置                                 │
│                                                             │
│  第4阶段: 在完整数据集上验证                                      │
│  ├── 使用: 最佳参数组合                                          │
│  ├── 训练: 完整训练 (100+ epochs)                                  │
│  └── 评估: 测试集性能 + 稳定性分析                              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## A.3 常见陷阱与解决方案

### A.3.1 训练问题

```
┌─────────────────────────────────────────────────────────────┐
│          常见训练问题与解决方案                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  问题1: KL 散度过大 (KL Domination)                             │
│  ├── 症状:                                                    │
│  │   ├── KL_loss >> NLL_loss                                      │
│  │   ├── 潜在空间快速收缩到 N(0,1)                               │
│  │   └── 重构质量极差                                           │
│  ├── 原因:                                                    │
│  │   ├── KL 退火太快 (k 太大)                                    │
│  │   ├── 潜在维度太小 (无法表达信息)                             │
│  │   └── NLL_weight 太小                                         │
│  ├── 解决方案:                                                  │
│  │   ├── 减小 KL 退火速率: k = 0.0005 (而非 0.005)                │
│  │   ├── 增大潜在维度: latent_size = 8 → 16                        │
│  │   ├── 增大重构权重: nll_weight = 1.0 → 1.5                    │
│  │   └── 监控: 确保 KL/NLL 比值在合理范围 (0.1-0.5)              │
│                                                             │
│  问题2: 后验塌缩 (Posterior Collapse)                              │
│  ├── 症状:                                                    │
│  │   ├── KL_loss ≈ 0 (几乎为0)                                   │
│  │   ├── 潜在维度无变化 (所有样本映射到相同区域)                     │
│  │   └── 生成质量差，缺乏多样性                                   │
│  ├── 原因:                                                    │
│  │   ├── 潜在维度太大                                             │
│  │   ├── 解码器太强 (能从任何 z 重构)                             │
│  │   └── 缺乏辅助任务约束                                       │
│  ├── 解决方案:                                                  │
│  │   ├── 增大 KL 权重: β 更快增长到 1                             │
│  │   ├── 添加辅助任务: latent_label_weight = 0.01-0.1                │
│  │   ├── 降低解码器容量: hidden_size = 128 → 64                      │
│  │   └── 调整瓶颈: latent_size = 16 → 8                           │
│                                                             │
│  问题3: 重构不收敛                                              │
│  ├── 症状:                                                    │
│  │   ├── NLL_loss 震荡不降                                        │
│  │   ├── 重构输出与输入相差很大                                     │
│  │   └── KL_loss 也在波动                                       │
│  ├── 原因:                                                    │
│  │   ├── 学习率太大                                               │
│  │   ├── batch_size 太小                                         │
│  │   ├── 数据未归一化                                             │
│  │   └── 编码器和解码器能力不匹配                                │
│  ├── 解决方案:                                                  │
│  │   ├── 降低学习率: lr = 0.001 → 0.0005                            │
│  │   ├── 增大 batch_size: 32 → 64                                 │
│  │   ├── 检查数据归一化: 确保 scale=(0,1) 或 scale=(-1,1)            │
│  │   ├── 调整架构: 编码器和解码器 hidden_size 一致                  │
│  │   └── 使用学习率调度: ReduceLROnPlateau                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### A.3.2 数据相关问题

```
┌─────────────────────────────────────────────────────────────┐
│          数据相关常见问题                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  问题1: 数据泄漏 (Data Leakage)                                 │
│  ├── 症状: 验证集/测试集异常准确率接近 100%                      │
│  ├── 原因:                                                    │
│  │   ├── 训练集和测试集有重叠样本                                  │
│  │   ├── 同一电池的多时间窗口混入训练和测试                        │
│  │   └── 未来信息泄漏到训练 (未正确分割)                          │
│  ├── 解决方案:                                                  │
│  │   ├── 按电池ID分割: 不同电池的训练/测试集完全不重叠                     │
│  │   ├── 按时间分割: 测试集时间 > 训练集时间                       │
│  │   ├── 使用 Group K-Fold: 同一电池的样本在同一 fold                  │
│  │   └── 验证: 检查训练集和测试集是否有相似序列                    │
│                                                             │
│  问题2: 类别不平衡                                              │
│  ├── 症状:                                                    │
│  │   ├── 模型总是预测"正常"                                       │
│  │   ├── 召回率极低 (异常样本几乎检不出)                          │
│  │   └── 阈值难以确定                                          │
│  ├── 原因:                                                    │
│  │   ├── 正常样本 >> 异常样本 (通常 1000:1)                         │
│  │   └── 模型学会以降低总体 loss 为主                               │
│  ├── 解决方案:                                                  │
│  │   ├── 阈值调整: 降低阈值以提高召回率                             │
│  │   ├── 异常采样: 对异常样本过采样                                │
│  │   ├── Focal Loss: 增加异常样本权重                              │
│  │   └── 注意: 异常检测场景下不平衡是正常的，重点在召回率              │
│                                                             │
│  问题3: 数据质量问题                                              │
│  ├── 症状:                                                    │
│  │   ├── 训练 loss 下降但检测效果差                                 │
│  │   ├── 同类电池结果差异大                                       │
│  │   └── 某些特征异常但模型不敏感                                 │
│  ├── 原因:                                                    │
│  │   ├── 传感器噪声未过滤                                         │
│  │   ├── 缺失值处理不当                                          │
│  │   ├── 异常值未处理                                           │
│  │   └── 特征归一化不一致                                         │
│  ├── 解决方案:                                                  │
│  │   ├── 数据清洗: 移除明显噪声和异常值                             │
│  │   ├── 特征工程: 滑动平均、差分、统计量                          │
│  │   ├── 归一化: 每个特征独立归一化                              │
│  │   ├── 数据增强: 对正常样本添加小幅噪声模拟真实变化                   │
│  │   └── 可视化: 检查训练数据和预测数据分布                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## A.4 不同数据规模的推荐配置

### A.4.1 小规模数据 (< 10K 样本)

```python
# 小规模数据配置
config_small = {
    # 模型规模 - 防止过拟合
    'hidden_size': 64,              # 减小模型容量
    'latent_size': 4,              # 更小的瓶颈

    # 训练策略 - 充分利用数据
    'epochs': 200,                  # 更多轮次
    'early_stopping': 30,            # 更大 patience

    # 数据增强 - 增加数据多样性
    'data_augmentation': True,
    'augmentation_strength': 0.1,  # 轻微增强

    # 正则化 - 防止过拟合
    'dropout': 0.2,
    'weight_decay': 1e-4,

    # KL 退火 - 更慢地引入约束
    'kl_anneal0': 20000,           # 更长的退火周期
    'kl_k': 0.0005,                 # 更小的增长速率
}
```

---

### A.4.2 中规模数据 (10K - 100K 样本)

```python
# 中规模数据配置 (推荐)
config_medium = {
    # 模型规模 - 平衡表达和泛化
    'hidden_size': 128,             # 标准配置
    'latent_size': 8,               # 标准配置

    # 训练策略 - 标准训练
    'epochs': 100,
    'early_stopping': 15,

    # 正则化 - 适度正则化
    'dropout': 0.1,
    'weight_decay': 1e-5,

    # KL 退火 - 标准 logistic
    'kl_anneal0': 1.0,
    'kl_k': 0.001,
    'kl_x0': 5000,                  # 总步数的 50%

    # 其他
    'batch_size': 64,
    'learning_rate': 0.001,
}
```

---

### A.4.3 大规模数据 (> 100K 样本)

```python
# 大规模数据配置
config_large = {
    # 模型规模 - 增强表达能力
    'hidden_size': 256,             # 更大模型
    'latent_size': 16,              # 更大瓶颈

    # 训练策略 - 快速训练
    'epochs': 50,                    # 较少轮次
    'early_stopping': 10,

    # 正则化 - 减少正则化
    'dropout': 0.05,                 # 轻微 dropout
    'weight_decay': 1e-6,

    # KL 退火 - 更快达到满权重
    'kl_anneal0': 5000,             # 更短的退火
    'kl_k': 0.002,                  # 更快的增长

    # 批处理 - 充分利用 GPU
    'batch_size': 128,                # 更大 batch
    'gradient_accumulation': 2,      # 模拟更大 batch

    # 学习率 - 稍大学习率
    'learning_rate': 0.002,
    'lr_schedule': 'cosine',       # 余弦退火
}
```

---

### A.4.4 配置对比表

| 配置项 | 小规模 | 中规模 | 大规模 |
|---------|-------|--------|-------|
| **hidden_size** | 64 | 128 | 256 |
| **latent_size** | 4 | 8 | 16 |
| **epochs** | 200 | 100 | 50 |
| **batch_size** | 32 | 64 | 128 |
| **learning_rate** | 0.0005 | 0.001 | 0.002 |
| **dropout** | 0.2 | 0.1 | 0.05 |
| **kl_anneal0** | 20000 | 10000 | 5000 |
| **weight_decay** | 1e-4 | 1e-5 | 1e-6 |

---

## 附录小结

学习完本附录后，您应该能够：

- [ ] 理解 DyAD 各个超参数的作用和影响
- [ ] 掌握系统性的超参数搜索策略
- [ ] 识别和解决常见的训练问题
- [ ] 根据数据规模选择合适的配置
- [ ] 独立进行 DyAD 模型的调参工作

---

## 快速参考卡片

```
┌─────────────────────────────────────────────────────────────┐
│           快速调参参考卡片                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  问题诊断 → 快速解决方案:                                     │
│  ───────────────────────────────────────────────────────       │
│  KL 过大 → 减小退火速率 k, 增大 latent_size                 │
│  KL 过小 → 增大退火速率 k, 减小 latent_size                 │
│  重构差 → 增大 nll_weight, 检查编码器能力                    │
│  不收敛   → 降低学习率, 增大 batch_size                        │
│  过拟合   → 增加 dropout, weight_decay                             │
│  欠拟合   → 减小正则化, 增加模型容量, 训练更久            │
│  后验塌缩 → 增大 latent_label_weight, 减小解码器容量            │
│                                                             │
│  推荐默认配置 (中规模数据):                                  │
│  ───────────────────────────────────────────────────────       │
│  hidden_size: 128                                            │
│  latent_size: 8                                              │
│  learning_rate: 0.001                                         │
│  batch_size: 64                                               │
│  nll_weight: 1.0                                             │
│  latent_label_weight: 0.01                                    │
│  kl_anneal0: 10000 (logistic)                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 参考文献

1. "Hyperparameter Optimization for Deep Learning" - Bengio, 2021
2. "On the Importance of Initialization and Scaling" - LeCun, 2015
3. "Bayesian Optimization for Hyperparameter Tuning" - Snoek, 2012

---

**回到教程目录**: [README.md](./README.md)

---

**附录版本**: v1.0
**最后更新**: 2026-02-12
**来源**: 基于多个项目实践经验总结
